{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9d3efb3eca843a49b6ebd3f31694a5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c3c3b94822841d5b4c420cc99c6f8c7",
              "IPY_MODEL_b305ba3d1e574bfb923b79b4b1dd885e",
              "IPY_MODEL_18ea125d76a24b34b670636cb6de0ab3"
            ],
            "layout": "IPY_MODEL_11b072dc036b42fcbe6305fe7c9f1782"
          }
        },
        "8c3c3b94822841d5b4c420cc99c6f8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09daa87c07c146f6a0b7f3e20131d7ca",
            "placeholder": "​",
            "style": "IPY_MODEL_beae36bd1a5d4781bf97d2cdaf7d65a8",
            "value": "config.json: 100%"
          }
        },
        "b305ba3d1e574bfb923b79b4b1dd885e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d253120f9dc455f883aed0146aa688a",
            "max": 1520,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5902ca74e65741479916cab985d2a1b8",
            "value": 1520
          }
        },
        "18ea125d76a24b34b670636cb6de0ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df9b0175c88d4df29aec19934c4d923b",
            "placeholder": "​",
            "style": "IPY_MODEL_9ce2e3165da74d47a16244c636a84f58",
            "value": " 1.52k/1.52k [00:00&lt;00:00, 92.6kB/s]"
          }
        },
        "11b072dc036b42fcbe6305fe7c9f1782": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09daa87c07c146f6a0b7f3e20131d7ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "beae36bd1a5d4781bf97d2cdaf7d65a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d253120f9dc455f883aed0146aa688a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5902ca74e65741479916cab985d2a1b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df9b0175c88d4df29aec19934c4d923b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ce2e3165da74d47a16244c636a84f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb30a1d2d7ca4321a0a388e4583c9df7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_adc6327d05404093ba07a390060dac22",
              "IPY_MODEL_02c9eef5c2f44a32b3832158f8a037d6",
              "IPY_MODEL_53e0ca10ab644c5cb03126584a364e9d"
            ],
            "layout": "IPY_MODEL_ca720202593b49839baea9592ea66b77"
          }
        },
        "adc6327d05404093ba07a390060dac22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a89a2c8b556342dc8c829ee790405334",
            "placeholder": "​",
            "style": "IPY_MODEL_10a947ea5d13489baa085a3d9d9d515b",
            "value": "model.safetensors: 100%"
          }
        },
        "02c9eef5c2f44a32b3832158f8a037d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8a1044f06d24b0883ee7af4139239a3",
            "max": 1027676737,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f003d4d15f642549db84a33e0ae6ccc",
            "value": 1027676737
          }
        },
        "53e0ca10ab644c5cb03126584a364e9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62b15076c5794693a4945cf125b8558f",
            "placeholder": "​",
            "style": "IPY_MODEL_b0c92cf628a74108b02d6f6231d77d9b",
            "value": " 1.03G/1.03G [00:24&lt;00:00, 42.5MB/s]"
          }
        },
        "ca720202593b49839baea9592ea66b77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a89a2c8b556342dc8c829ee790405334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10a947ea5d13489baa085a3d9d9d515b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8a1044f06d24b0883ee7af4139239a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f003d4d15f642549db84a33e0ae6ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62b15076c5794693a4945cf125b8558f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0c92cf628a74108b02d6f6231d77d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aae277d6c2414a47a58c746c4827ef9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f22b90dacdf42ba92da453d970af0e0",
              "IPY_MODEL_cc4df9dafa874a5badc6dad6987c794e",
              "IPY_MODEL_891681f7a6fc45478ba247388143c598"
            ],
            "layout": "IPY_MODEL_025d6114703c41ee956113473424e3ae"
          }
        },
        "0f22b90dacdf42ba92da453d970af0e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f91dd130133648da9e9f221e3d66b045",
            "placeholder": "​",
            "style": "IPY_MODEL_9135352006e14526b5dcc3c6a24fd177",
            "value": "generation_config.json: 100%"
          }
        },
        "cc4df9dafa874a5badc6dad6987c794e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e52d28fe4de64a3a8ccfc8a987c0711a",
            "max": 234,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4039742297f248eea1cc9cb69c8c8ab7",
            "value": 234
          }
        },
        "891681f7a6fc45478ba247388143c598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfaeaa3ccaed45b3bb364ed0b3b794b9",
            "placeholder": "​",
            "style": "IPY_MODEL_142de0e017e644cb83744b10bcd48649",
            "value": " 234/234 [00:00&lt;00:00, 22.9kB/s]"
          }
        },
        "025d6114703c41ee956113473424e3ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f91dd130133648da9e9f221e3d66b045": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9135352006e14526b5dcc3c6a24fd177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e52d28fe4de64a3a8ccfc8a987c0711a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4039742297f248eea1cc9cb69c8c8ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bfaeaa3ccaed45b3bb364ed0b3b794b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "142de0e017e644cb83744b10bcd48649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb5bbf18e83948d794fdaa530f529d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1530b354f344b7387e5df8c4467c208",
              "IPY_MODEL_546920513edf4402beda96c3a7db80d7",
              "IPY_MODEL_ae7f6250e6de4859adbe05a1bc7f20af"
            ],
            "layout": "IPY_MODEL_4c7ffc4ba13448589a2a22226fe2a925"
          }
        },
        "a1530b354f344b7387e5df8c4467c208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_885c0432ccf94d7a97bc22310a58a624",
            "placeholder": "​",
            "style": "IPY_MODEL_fcaf3cbe96f24e15be8446d39f4dd7f5",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "546920513edf4402beda96c3a7db80d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdc833e684b448199a3b9bb297c3c397",
            "max": 54674,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7aa5a27f2e9f4ff3afa8e4f4a11b07e8",
            "value": 54674
          }
        },
        "ae7f6250e6de4859adbe05a1bc7f20af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48a2c87144d148eb9d10eeec101293ec",
            "placeholder": "​",
            "style": "IPY_MODEL_89821fa693da461c823adb73ca09f937",
            "value": " 54.7k/54.7k [00:00&lt;00:00, 3.53MB/s]"
          }
        },
        "4c7ffc4ba13448589a2a22226fe2a925": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "885c0432ccf94d7a97bc22310a58a624": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcaf3cbe96f24e15be8446d39f4dd7f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdc833e684b448199a3b9bb297c3c397": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aa5a27f2e9f4ff3afa8e4f4a11b07e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48a2c87144d148eb9d10eeec101293ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89821fa693da461c823adb73ca09f937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df4d2580bb5c4bce865d12cf4986bf3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bf7adf5027242a59295324135cd2e20",
              "IPY_MODEL_3f696ee8157840039c3fbcf32c2dfed2",
              "IPY_MODEL_5d5187a290be4a8e86a41e5eb696b134"
            ],
            "layout": "IPY_MODEL_3fafbd292bb140bdbc867148590e2a00"
          }
        },
        "6bf7adf5027242a59295324135cd2e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c7b7f05aaa6403c807416bae3ada0c4",
            "placeholder": "​",
            "style": "IPY_MODEL_1ed9fce664474886982e44afe665f1f1",
            "value": "tokenizer.json: 100%"
          }
        },
        "3f696ee8157840039c3fbcf32c2dfed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a172f94b674a4646b34b049959c786b3",
            "max": 17209920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2efe7116dd2a425da06982e74656ace1",
            "value": 17209920
          }
        },
        "5d5187a290be4a8e86a41e5eb696b134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40ddacea4da14b8394549c91345e0656",
            "placeholder": "​",
            "style": "IPY_MODEL_cb9b2058317c416196d2839d774fcd31",
            "value": " 17.2M/17.2M [00:00&lt;00:00, 43.2MB/s]"
          }
        },
        "3fafbd292bb140bdbc867148590e2a00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c7b7f05aaa6403c807416bae3ada0c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ed9fce664474886982e44afe665f1f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a172f94b674a4646b34b049959c786b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2efe7116dd2a425da06982e74656ace1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40ddacea4da14b8394549c91345e0656": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb9b2058317c416196d2839d774fcd31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81c079cec188459a9494affcaff64f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14dff2a1a73a4555ae569137ed5d5402",
              "IPY_MODEL_920015a3491c426ca9ecc988bbafbebe",
              "IPY_MODEL_7552117f013f41e3b70ab8640074e2ef"
            ],
            "layout": "IPY_MODEL_c74056539b1f4069a79606f6bcb0f975"
          }
        },
        "14dff2a1a73a4555ae569137ed5d5402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97f8eadf79aa4013a66032447a715391",
            "placeholder": "​",
            "style": "IPY_MODEL_06913f4ae319440ea48b90e919822ff6",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "920015a3491c426ca9ecc988bbafbebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77fbf69e377e43248dd300f503863e78",
            "max": 454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a0c220aa7cd4b799102af803d09025b",
            "value": 454
          }
        },
        "7552117f013f41e3b70ab8640074e2ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3966043a52544e79a52bcc69cdf1f8ce",
            "placeholder": "​",
            "style": "IPY_MODEL_e610e267ce374e319184427e347f69f5",
            "value": " 454/454 [00:00&lt;00:00, 46.3kB/s]"
          }
        },
        "c74056539b1f4069a79606f6bcb0f975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97f8eadf79aa4013a66032447a715391": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06913f4ae319440ea48b90e919822ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77fbf69e377e43248dd300f503863e78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a0c220aa7cd4b799102af803d09025b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3966043a52544e79a52bcc69cdf1f8ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e610e267ce374e319184427e347f69f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb9b013fdc144e2f8f4dae54b9be75cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5a8892845f14c27842da3728c8d1e2b",
              "IPY_MODEL_33942bc64379494a9e1dc475ba8f7154",
              "IPY_MODEL_48d3efae8418461282ccfa6d2dd61821"
            ],
            "layout": "IPY_MODEL_1b75df199f0242b8886e06c2501ac7b8"
          }
        },
        "f5a8892845f14c27842da3728c8d1e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fbf42d82f594063966921d2c418978b",
            "placeholder": "​",
            "style": "IPY_MODEL_5d076c03989d4142a9d8ae3c5c42accf",
            "value": "unified_chip2.jsonl: 100%"
          }
        },
        "33942bc64379494a9e1dc475ba8f7154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e4a2d181af1495f8a8508bca8c4c61e",
            "max": 95645860,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3324e4f359b047df945546cbe8bef79a",
            "value": 95645860
          }
        },
        "48d3efae8418461282ccfa6d2dd61821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acb7d15c65f047eb9163e7af82ce9af7",
            "placeholder": "​",
            "style": "IPY_MODEL_6def145ab4c242bcb5bec49c990cf707",
            "value": " 95.6M/95.6M [00:00&lt;00:00, 171MB/s]"
          }
        },
        "1b75df199f0242b8886e06c2501ac7b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fbf42d82f594063966921d2c418978b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d076c03989d4142a9d8ae3c5c42accf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e4a2d181af1495f8a8508bca8c4c61e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3324e4f359b047df945546cbe8bef79a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "acb7d15c65f047eb9163e7af82ce9af7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6def145ab4c242bcb5bec49c990cf707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d816f6351c3947509db0a9de661149af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a478e4dc6c84b92b3b24ba2ebf4d027",
              "IPY_MODEL_3efa55ba7c1443b9b5da3e591ae5896b",
              "IPY_MODEL_38e4318e322140cf9ff4220042e9b776"
            ],
            "layout": "IPY_MODEL_594be35d0f3d4f47afa740ccb72212ad"
          }
        },
        "1a478e4dc6c84b92b3b24ba2ebf4d027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f2517aaebfd41e29c12e482564dc018",
            "placeholder": "​",
            "style": "IPY_MODEL_4eeddf5b5618459f88a0feb1149b6844",
            "value": "Generating train split: "
          }
        },
        "3efa55ba7c1443b9b5da3e591ae5896b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85d6ea21dd124ec28b7da225b4178f32",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ceedb95b3454cebb7df1eea855bc553",
            "value": 1
          }
        },
        "38e4318e322140cf9ff4220042e9b776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fea4453e5d674b6ea2ea505853c9f085",
            "placeholder": "​",
            "style": "IPY_MODEL_fc500718452a40609a416e6f249560a2",
            "value": " 210289/0 [00:00&lt;00:00, 286345.72 examples/s]"
          }
        },
        "594be35d0f3d4f47afa740ccb72212ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f2517aaebfd41e29c12e482564dc018": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eeddf5b5618459f88a0feb1149b6844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85d6ea21dd124ec28b7da225b4178f32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1ceedb95b3454cebb7df1eea855bc553": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fea4453e5d674b6ea2ea505853c9f085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc500718452a40609a416e6f249560a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Datbwoyyy/unsloth/blob/main/REMOVE_BREAKS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WUlRvA8WaImL",
        "outputId": "1471220a-181d-4950-f4f3-745bda8dee36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.2.12-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth_zoo>=2025.2.5 (from unsloth)\n",
            "  Downloading unsloth_zoo-2025.2.5-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.5.1+cu124)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting bitsandbytes (from unsloth)\n",
            "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.9.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.48.3)\n",
            "Collecting datasets>=2.16.0 (from unsloth)\n",
            "  Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.3.0)\n",
            "Collecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\n",
            "  Downloading trl-0.15.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.14.0)\n",
            "Collecting protobuf<4.0.0 (from unsloth)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.28.1)\n",
            "Collecting hf_transfer (from unsloth)\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.32.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.20.1+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Collecting xxhash (from datasets>=2.16.0->unsloth)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.16.0->unsloth)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.11.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (13.9.4)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.2.5->unsloth)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.2.5->unsloth) (11.1.0)\n",
            "Collecting torch>=2.4.0 (from unsloth)\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting triton>=3.0.0 (from unsloth)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.6.1)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from unsloth)\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n",
            "Downloading unsloth-2025.2.12-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.6/187.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.1-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.15.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.2.5-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (43.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.14-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, xxhash, shtab, protobuf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf_transfer, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tyro, nvidia-cusolver-cu12, torch, datasets, xformers, torchvision, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu124\n",
            "    Uninstalling torchvision-0.20.1+cu124:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.2 cut_cross_entropy-25.1.1 datasets-3.3.1 dill-0.3.8 hf_transfer-0.1.9 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nvjitlink-cu12-12.4.127 protobuf-3.20.3 shtab-1.7.1 torch-2.6.0 torchvision-0.21.0 triton-3.2.0 trl-0.15.1 tyro-0.9.14 unsloth-2025.2.12 unsloth_zoo-2025.2.5 xformers-0.0.29.post3 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "0eb16c00233e47d3bee17052db145cf2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install unsloth --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y cuda-toolkit-12-1\n",
        "!export PATH=/usr/local/cuda-12.1/bin${PATH:+:${PATH}}\n",
        "!export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEH6gf-gaqhA",
        "outputId": "e3818c45-dc31-4841-f895-986c4eadef00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cuda-cccl-12-1 cuda-command-line-tools-12-1 cuda-compiler-12-1 cuda-cudart-12-1\n",
            "  cuda-cudart-dev-12-1 cuda-cuobjdump-12-1 cuda-cupti-12-1 cuda-cupti-dev-12-1 cuda-cuxxfilt-12-1\n",
            "  cuda-documentation-12-1 cuda-driver-dev-12-1 cuda-gdb-12-1 cuda-libraries-12-1\n",
            "  cuda-libraries-dev-12-1 cuda-nsight-12-1 cuda-nsight-compute-12-1 cuda-nsight-systems-12-1\n",
            "  cuda-nvcc-12-1 cuda-nvdisasm-12-1 cuda-nvml-dev-12-1 cuda-nvprof-12-1 cuda-nvprune-12-1\n",
            "  cuda-nvrtc-12-1 cuda-nvrtc-dev-12-1 cuda-nvtx-12-1 cuda-nvvp-12-1 cuda-opencl-12-1\n",
            "  cuda-opencl-dev-12-1 cuda-profiler-api-12-1 cuda-sanitizer-12-1 cuda-toolkit-12-1-config-common\n",
            "  cuda-tools-12-1 cuda-visual-tools-12-1 default-jre default-jre-headless fonts-dejavu-core\n",
            "  fonts-dejavu-extra gds-tools-12-1 libatk-wrapper-java libatk-wrapper-java-jni libcublas-12-1\n",
            "  libcublas-dev-12-1 libcufft-12-1 libcufft-dev-12-1 libcufile-12-1 libcufile-dev-12-1\n",
            "  libcurand-12-1 libcurand-dev-12-1 libcusolver-12-1 libcusolver-dev-12-1 libcusparse-12-1\n",
            "  libcusparse-dev-12-1 libfontenc1 libnpp-12-1 libnpp-dev-12-1 libnvjitlink-12-1\n",
            "  libnvjitlink-dev-12-1 libnvjpeg-12-1 libnvjpeg-dev-12-1 libnvvm-samples-12-1 libtinfo5\n",
            "  libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0 libxcb-util1 libxcb-xinerama0\n",
            "  libxcb-xinput0 libxcb-xkb1 libxkbcommon-x11-0 libxkbfile1 libxtst6 libxxf86dga1\n",
            "  nsight-compute-2023.1.1 nsight-systems-2023.1.2 openjdk-11-jre x11-utils\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  cuda-cccl-12-1 cuda-command-line-tools-12-1 cuda-compiler-12-1 cuda-cudart-12-1\n",
            "  cuda-cudart-dev-12-1 cuda-cuobjdump-12-1 cuda-cupti-12-1 cuda-cupti-dev-12-1 cuda-cuxxfilt-12-1\n",
            "  cuda-documentation-12-1 cuda-driver-dev-12-1 cuda-gdb-12-1 cuda-libraries-12-1\n",
            "  cuda-libraries-dev-12-1 cuda-nsight-12-1 cuda-nsight-compute-12-1 cuda-nsight-systems-12-1\n",
            "  cuda-nvcc-12-1 cuda-nvdisasm-12-1 cuda-nvml-dev-12-1 cuda-nvprof-12-1 cuda-nvprune-12-1\n",
            "  cuda-nvrtc-12-1 cuda-nvrtc-dev-12-1 cuda-nvtx-12-1 cuda-nvvp-12-1 cuda-opencl-12-1\n",
            "  cuda-opencl-dev-12-1 cuda-profiler-api-12-1 cuda-sanitizer-12-1 cuda-toolkit-12-1\n",
            "  cuda-toolkit-12-1-config-common cuda-tools-12-1 cuda-visual-tools-12-1 default-jre\n",
            "  default-jre-headless fonts-dejavu-core fonts-dejavu-extra gds-tools-12-1 libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libcublas-12-1 libcublas-dev-12-1 libcufft-12-1 libcufft-dev-12-1\n",
            "  libcufile-12-1 libcufile-dev-12-1 libcurand-12-1 libcurand-dev-12-1 libcusolver-12-1\n",
            "  libcusolver-dev-12-1 libcusparse-12-1 libcusparse-dev-12-1 libfontenc1 libnpp-12-1\n",
            "  libnpp-dev-12-1 libnvjitlink-12-1 libnvjitlink-dev-12-1 libnvjpeg-12-1 libnvjpeg-dev-12-1\n",
            "  libnvvm-samples-12-1 libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0\n",
            "  libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxkbcommon-x11-0 libxkbfile1 libxtst6\n",
            "  libxxf86dga1 nsight-compute-2023.1.1 nsight-systems-2023.1.2 openjdk-11-jre x11-utils\n",
            "0 upgraded, 78 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 2,764 MB of archives.\n",
            "After this operation, 6,339 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre-headless amd64 2:1.11-72build2 [3,042 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cccl-12-1 12.1.109-1 [1,060 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.26+4-1ubuntu1~22.04 [214 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-12-1 12.1.105-1 [16.6 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 default-jre amd64 2:1.11-72build2 [896 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libtinfo5 amd64 6.3-2ubuntu0.1 [100 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:25 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cupti-dev-12-1 12.1.105-1 [2,553 kB]\n",
            "Get:26 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvdisasm-12-1 12.1.105-1 [49.9 MB]\n",
            "Get:27 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuobjdump-12-1 12.1.111-1 [173 kB]\n",
            "Get:28 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-gdb-12-1 12.1.105-1 [4,538 kB]\n",
            "Get:29 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprof-12-1 12.1.105-1 [2,435 kB]\n",
            "Get:30 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvtx-12-1 12.1.105-1 [51.4 kB]\n",
            "Get:31 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-sanitizer-12-1 12.1.105-1 [8,921 kB]\n",
            "Get:32 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-command-line-tools-12-1 12.1.1-1 [2,542 B]\n",
            "Get:33 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cuxxfilt-12-1 12.1.105-1 [190 kB]\n",
            "Get:34 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-12-1-config-common 12.1.105-1 [16.3 kB]\n",
            "Get:35 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-12-1 12.1.105-1 [158 kB]\n",
            "Get:36 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-driver-dev-12-1 12.1.105-1 [27.8 kB]\n",
            "Get:37 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-cudart-dev-12-1 12.1.105-1 [894 kB]\n",
            "Get:38 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvcc-12-1 12.1.105-1 [44.7 MB]\n",
            "Get:39 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvprune-12-1 12.1.105-1 [58.5 kB]\n",
            "Get:40 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-compiler-12-1 12.1.1-1 [2,506 B]\n",
            "Get:41 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-documentation-12-1 12.1.105-1 [49.9 kB]\n",
            "Get:42 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-12-1 12.1.105-1 [16.8 MB]\n",
            "Get:43 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-opencl-12-1 12.1.105-1 [23.8 kB]\n",
            "Get:44 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-12-1 12.1.3.1-1 [244 MB]\n",
            "Get:45 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-12-1 11.0.2.54-1 [59.9 MB]\n",
            "Get:46 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-12-1 1.6.1.9-1 [643 kB]\n",
            "Get:47 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-12-1 10.3.2.106-1 [41.5 MB]\n",
            "Get:48 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-12-1 11.4.5.107-1 [76.2 MB]\n",
            "Get:49 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-12-1 12.1.0.106-1 [108 MB]\n",
            "Get:50 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-12-1 12.1.0.40-1 [95.2 MB]\n",
            "Get:51 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjitlink-12-1 12.1.105-1 [14.7 MB]\n",
            "Get:52 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-12-1 12.2.0.2-1 [2,285 kB]\n",
            "Get:53 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-12-1 12.1.1-1 [2,598 B]\n",
            "Get:54 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-profiler-api-12-1 12.1.105-1 [18.5 kB]\n",
            "Get:55 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvrtc-dev-12-1 12.1.105-1 [13.9 MB]\n",
            "Get:56 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-opencl-dev-12-1 12.1.105-1 [70.3 kB]\n",
            "Get:57 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-dev-12-1 12.1.3.1-1 [255 MB]\n",
            "Get:58 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufft-dev-12-1 11.0.2.54-1 [117 MB]\n",
            "Get:59 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcufile-dev-12-1 1.6.1.9-1 [1,568 kB]\n",
            "Get:60 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-dev-12-1 10.3.2.106-1 [41.7 MB]\n",
            "Get:61 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-dev-12-1 11.4.5.107-1 [49.2 MB]\n",
            "Get:62 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-dev-12-1 12.1.0.106-1 [112 MB]\n",
            "Get:63 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnpp-dev-12-1 12.1.0.40-1 [92.7 MB]\n",
            "Get:64 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjitlink-dev-12-1 12.1.105-1 [11.4 MB]\n",
            "Get:65 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvjpeg-dev-12-1 12.2.0.2-1 [1,962 kB]\n",
            "Get:66 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-libraries-dev-12-1 12.1.1-1 [2,638 B]\n",
            "Get:67 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-12-1 12.1.105-1 [119 MB]\n",
            "Get:68 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-compute-2023.1.1 2023.1.1.4-1 [703 MB]\n",
            "Get:69 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-compute-12-1 12.1.1-1 [4,056 B]\n",
            "Get:70 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-systems-2023.1.2 2023.1.2.43-32377213v0 [297 MB]\n",
            "Get:71 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nsight-systems-12-1 12.1.1-1 [3,424 B]\n",
            "Get:72 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvml-dev-12-1 12.1.105-1 [83.0 kB]\n",
            "Get:73 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-nvvp-12-1 12.1.105-1 [113 MB]\n",
            "Get:74 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-visual-tools-12-1 12.1.1-1 [2,938 B]\n",
            "Get:75 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  gds-tools-12-1 1.6.1.9-1 [39.2 MB]\n",
            "Get:76 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-tools-12-1 12.1.1-1 [2,462 B]\n",
            "Get:77 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvvm-samples-12-1 12.1.105-1 [31.9 kB]\n",
            "Get:78 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-12-1 12.1.1-1 [3,308 B]\n",
            "Fetched 2,764 MB in 1min 34s (29.3 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package cuda-cccl-12-1.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-cccl-12-1_12.1.109-1_amd64.deb ...\n",
            "Unpacking cuda-cccl-12-1 (12.1.109-1) ...\n",
            "Selecting previously unselected package cuda-cupti-12-1.\n",
            "Preparing to unpack .../01-cuda-cupti-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-cupti-dev-12-1.\n",
            "Preparing to unpack .../02-cuda-cupti-dev-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-cupti-dev-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-nvdisasm-12-1.\n",
            "Preparing to unpack .../03-cuda-nvdisasm-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-nvdisasm-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-cuobjdump-12-1.\n",
            "Preparing to unpack .../04-cuda-cuobjdump-12-1_12.1.111-1_amd64.deb ...\n",
            "Unpacking cuda-cuobjdump-12-1 (12.1.111-1) ...\n",
            "Selecting previously unselected package cuda-gdb-12-1.\n",
            "Preparing to unpack .../05-cuda-gdb-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-gdb-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-nvprof-12-1.\n",
            "Preparing to unpack .../06-cuda-nvprof-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-nvprof-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-nvtx-12-1.\n",
            "Preparing to unpack .../07-cuda-nvtx-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-nvtx-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-sanitizer-12-1.\n",
            "Preparing to unpack .../08-cuda-sanitizer-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-sanitizer-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-12-1.\n",
            "Preparing to unpack .../09-cuda-command-line-tools-12-1_12.1.1-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-12-1 (12.1.1-1) ...\n",
            "Selecting previously unselected package cuda-cuxxfilt-12-1.\n",
            "Preparing to unpack .../10-cuda-cuxxfilt-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-cuxxfilt-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-12-1-config-common.\n",
            "Preparing to unpack .../11-cuda-toolkit-12-1-config-common_12.1.105-1_all.deb ...\n",
            "Unpacking cuda-toolkit-12-1-config-common (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-cudart-12-1.\n",
            "Preparing to unpack .../12-cuda-cudart-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-12-1.\n",
            "Preparing to unpack .../13-cuda-driver-dev-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-12-1.\n",
            "Preparing to unpack .../14-cuda-cudart-dev-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-nvcc-12-1.\n",
            "Preparing to unpack .../15-cuda-nvcc-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-nvcc-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-nvprune-12-1.\n",
            "Preparing to unpack .../16-cuda-nvprune-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-nvprune-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-compiler-12-1.\n",
            "Preparing to unpack .../17-cuda-compiler-12-1_12.1.1-1_amd64.deb ...\n",
            "Unpacking cuda-compiler-12-1 (12.1.1-1) ...\n",
            "Selecting previously unselected package cuda-documentation-12-1.\n",
            "Preparing to unpack .../18-cuda-documentation-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-12-1.\n",
            "Preparing to unpack .../19-cuda-nvrtc-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-opencl-12-1.\n",
            "Preparing to unpack .../20-cuda-opencl-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-opencl-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package libcublas-12-1.\n",
            "Preparing to unpack .../21-libcublas-12-1_12.1.3.1-1_amd64.deb ...\n",
            "Unpacking libcublas-12-1 (12.1.3.1-1) ...\n",
            "Selecting previously unselected package libcufft-12-1.\n",
            "Preparing to unpack .../22-libcufft-12-1_11.0.2.54-1_amd64.deb ...\n",
            "Unpacking libcufft-12-1 (11.0.2.54-1) ...\n",
            "Selecting previously unselected package libcufile-12-1.\n",
            "Preparing to unpack .../23-libcufile-12-1_1.6.1.9-1_amd64.deb ...\n",
            "Unpacking libcufile-12-1 (1.6.1.9-1) ...\n",
            "Selecting previously unselected package libcurand-12-1.\n",
            "Preparing to unpack .../24-libcurand-12-1_10.3.2.106-1_amd64.deb ...\n",
            "Unpacking libcurand-12-1 (10.3.2.106-1) ...\n",
            "Selecting previously unselected package libcusolver-12-1.\n",
            "Preparing to unpack .../25-libcusolver-12-1_11.4.5.107-1_amd64.deb ...\n",
            "Unpacking libcusolver-12-1 (11.4.5.107-1) ...\n",
            "Selecting previously unselected package libcusparse-12-1.\n",
            "Preparing to unpack .../26-libcusparse-12-1_12.1.0.106-1_amd64.deb ...\n",
            "Unpacking libcusparse-12-1 (12.1.0.106-1) ...\n",
            "Selecting previously unselected package libnpp-12-1.\n",
            "Preparing to unpack .../27-libnpp-12-1_12.1.0.40-1_amd64.deb ...\n",
            "Unpacking libnpp-12-1 (12.1.0.40-1) ...\n",
            "Selecting previously unselected package libnvjitlink-12-1.\n",
            "Preparing to unpack .../28-libnvjitlink-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking libnvjitlink-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package libnvjpeg-12-1.\n",
            "Preparing to unpack .../29-libnvjpeg-12-1_12.2.0.2-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-12-1 (12.2.0.2-1) ...\n",
            "Selecting previously unselected package cuda-libraries-12-1.\n",
            "Preparing to unpack .../30-cuda-libraries-12-1_12.1.1-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-12-1 (12.1.1-1) ...\n",
            "Selecting previously unselected package cuda-profiler-api-12-1.\n",
            "Preparing to unpack .../31-cuda-profiler-api-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-profiler-api-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-12-1.\n",
            "Preparing to unpack .../32-cuda-nvrtc-dev-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-opencl-dev-12-1.\n",
            "Preparing to unpack .../33-cuda-opencl-dev-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-opencl-dev-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package libcublas-dev-12-1.\n",
            "Preparing to unpack .../34-libcublas-dev-12-1_12.1.3.1-1_amd64.deb ...\n",
            "Unpacking libcublas-dev-12-1 (12.1.3.1-1) ...\n",
            "Selecting previously unselected package libcufft-dev-12-1.\n",
            "Preparing to unpack .../35-libcufft-dev-12-1_11.0.2.54-1_amd64.deb ...\n",
            "Unpacking libcufft-dev-12-1 (11.0.2.54-1) ...\n",
            "Selecting previously unselected package libcufile-dev-12-1.\n",
            "Preparing to unpack .../36-libcufile-dev-12-1_1.6.1.9-1_amd64.deb ...\n",
            "Unpacking libcufile-dev-12-1 (1.6.1.9-1) ...\n",
            "Selecting previously unselected package libcurand-dev-12-1.\n",
            "Preparing to unpack .../37-libcurand-dev-12-1_10.3.2.106-1_amd64.deb ...\n",
            "Unpacking libcurand-dev-12-1 (10.3.2.106-1) ...\n",
            "Selecting previously unselected package libcusolver-dev-12-1.\n",
            "Preparing to unpack .../38-libcusolver-dev-12-1_11.4.5.107-1_amd64.deb ...\n",
            "Unpacking libcusolver-dev-12-1 (11.4.5.107-1) ...\n",
            "Selecting previously unselected package libcusparse-dev-12-1.\n",
            "Preparing to unpack .../39-libcusparse-dev-12-1_12.1.0.106-1_amd64.deb ...\n",
            "Unpacking libcusparse-dev-12-1 (12.1.0.106-1) ...\n",
            "Selecting previously unselected package libnpp-dev-12-1.\n",
            "Preparing to unpack .../40-libnpp-dev-12-1_12.1.0.40-1_amd64.deb ...\n",
            "Unpacking libnpp-dev-12-1 (12.1.0.40-1) ...\n",
            "Selecting previously unselected package libnvjitlink-dev-12-1.\n",
            "Preparing to unpack .../41-libnvjitlink-dev-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking libnvjitlink-dev-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package libnvjpeg-dev-12-1.\n",
            "Preparing to unpack .../42-libnvjpeg-dev-12-1_12.2.0.2-1_amd64.deb ...\n",
            "Unpacking libnvjpeg-dev-12-1 (12.2.0.2-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-12-1.\n",
            "Preparing to unpack .../43-cuda-libraries-dev-12-1_12.1.1-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-12-1 (12.1.1-1) ...\n",
            "Selecting previously unselected package default-jre-headless.\n",
            "Preparing to unpack .../44-default-jre-headless_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre-headless (2:1.11-72build2) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../45-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../46-openjdk-11-jre_11.0.26+4-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package default-jre.\n",
            "Preparing to unpack .../47-default-jre_2%3a1.11-72build2_amd64.deb ...\n",
            "Unpacking default-jre (2:1.11-72build2) ...\n",
            "Selecting previously unselected package cuda-nsight-12-1.\n",
            "Preparing to unpack .../48-cuda-nsight-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package nsight-compute-2023.1.1.\n",
            "Preparing to unpack .../49-nsight-compute-2023.1.1_2023.1.1.4-1_amd64.deb ...\n",
            "Unpacking nsight-compute-2023.1.1 (2023.1.1.4-1) ...\n",
            "Selecting previously unselected package cuda-nsight-compute-12-1.\n",
            "Preparing to unpack .../50-cuda-nsight-compute-12-1_12.1.1-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-compute-12-1 (12.1.1-1) ...\n",
            "Selecting previously unselected package libtinfo5:amd64.\n",
            "Preparing to unpack .../51-libtinfo5_6.3-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../52-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../53-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../54-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../55-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../56-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../57-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../58-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../59-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../60-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package nsight-systems-2023.1.2.\n",
            "Preparing to unpack .../61-nsight-systems-2023.1.2_2023.1.2.43-32377213v0_amd64.deb ...\n",
            "Unpacking nsight-systems-2023.1.2 (2023.1.2.43-32377213v0) ...\n",
            "Selecting previously unselected package cuda-nsight-systems-12-1.\n",
            "Preparing to unpack .../62-cuda-nsight-systems-12-1_12.1.1-1_amd64.deb ...\n",
            "Unpacking cuda-nsight-systems-12-1 (12.1.1-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-12-1.\n",
            "Preparing to unpack .../63-cuda-nvml-dev-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-nvvp-12-1.\n",
            "Preparing to unpack .../64-cuda-nvvp-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking cuda-nvvp-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-12-1.\n",
            "Preparing to unpack .../65-cuda-visual-tools-12-1_12.1.1-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-12-1 (12.1.1-1) ...\n",
            "Selecting previously unselected package gds-tools-12-1.\n",
            "Preparing to unpack .../66-gds-tools-12-1_1.6.1.9-1_amd64.deb ...\n",
            "Unpacking gds-tools-12-1 (1.6.1.9-1) ...\n",
            "Selecting previously unselected package cuda-tools-12-1.\n",
            "Preparing to unpack .../67-cuda-tools-12-1_12.1.1-1_amd64.deb ...\n",
            "Unpacking cuda-tools-12-1 (12.1.1-1) ...\n",
            "Selecting previously unselected package libnvvm-samples-12-1.\n",
            "Preparing to unpack .../68-libnvvm-samples-12-1_12.1.105-1_amd64.deb ...\n",
            "Unpacking libnvvm-samples-12-1 (12.1.105-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-12-1.\n",
            "Preparing to unpack .../69-cuda-toolkit-12-1_12.1.1-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-12-1 (12.1.1-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../70-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../71-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../72-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../73-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../74-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../75-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../76-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../77-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up nsight-compute-2023.1.1 (2023.1.1.4-1) ...\n",
            "Setting up cuda-documentation-12-1 (12.1.105-1) ...\n",
            "Setting up cuda-nvdisasm-12-1 (12.1.105-1) ...\n",
            "Setting up cuda-driver-dev-12-1 (12.1.105-1) ...\n",
            "Setting up default-jre-headless (2:1.11-72build2) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up cuda-profiler-api-12-1 (12.1.105-1) ...\n",
            "Setting up cuda-nsight-compute-12-1 (12.1.1-1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up cuda-cuxxfilt-12-1 (12.1.105-1) ...\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.26+4-1ubuntu1~22.04) ...\n",
            "Setting up cuda-cccl-12-1 (12.1.109-1) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up default-jre (2:1.11-72build2) ...\n",
            "Setting up cuda-nvtx-12-1 (12.1.105-1) ...\n",
            "Setting up cuda-nsight-12-1 (12.1.105-1) ...\n",
            "Setting up libnvvm-samples-12-1 (12.1.105-1) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up cuda-nvprof-12-1 (12.1.105-1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up gds-tools-12-1 (1.6.1.9-1) ...\n",
            "Setting up cuda-toolkit-12-1-config-common (12.1.105-1) ...\n",
            "Setting alternatives\n",
            "Setting up libcusparse-12-1 (12.1.0.106-1) ...\n",
            "Setting up cuda-cuobjdump-12-1 (12.1.111-1) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up cuda-nvrtc-12-1 (12.1.105-1) ...\n",
            "Setting up cuda-sanitizer-12-1 (12.1.105-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up cuda-cupti-12-1 (12.1.105-1) ...\n",
            "Setting up libnpp-12-1 (12.1.0.40-1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libnvjitlink-12-1 (12.1.105-1) ...\n",
            "Setting up cuda-nvml-dev-12-1 (12.1.105-1) ...\n",
            "Setting up libnpp-dev-12-1 (12.1.0.40-1) ...\n",
            "Setting up cuda-opencl-12-1 (12.1.105-1) ...\n",
            "Setting up libcurand-12-1 (10.3.2.106-1) ...\n",
            "Setting up cuda-nvprune-12-1 (12.1.105-1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up cuda-nvrtc-dev-12-1 (12.1.105-1) ...\n",
            "Setting up cuda-gdb-12-1 (12.1.105-1) ...\n",
            "Setting up libtinfo5:amd64 (6.3-2ubuntu0.1) ...\n",
            "Setting up cuda-nvvp-12-1 (12.1.105-1) ...\n",
            "Setting up libnvjitlink-dev-12-1 (12.1.105-1) ...\n",
            "Setting up nsight-systems-2023.1.2 (2023.1.2.43-32377213v0) ...\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.1.2/target-linux-x64/nsys to provide /usr/local/bin/nsys (nsys) in auto mode\n",
            "update-alternatives: using /opt/nvidia/nsight-systems/2023.1.2/host-linux-x64/nsys-ui to provide /usr/local/bin/nsys-ui (nsys-ui) in auto mode\n",
            "Setting up libnvjpeg-12-1 (12.2.0.2-1) ...\n",
            "Setting up libcusolver-12-1 (11.4.5.107-1) ...\n",
            "Setting up libcufile-12-1 (1.6.1.9-1) ...\n",
            "Setting alternatives\n",
            "Setting up cuda-nsight-systems-12-1 (12.1.1-1) ...\n",
            "Setting up libcusparse-dev-12-1 (12.1.0.106-1) ...\n",
            "Setting up libcurand-dev-12-1 (10.3.2.106-1) ...\n",
            "Setting up libcusolver-dev-12-1 (11.4.5.107-1) ...\n",
            "Setting up cuda-cudart-12-1 (12.1.105-1) ...\n",
            "Setting up cuda-cupti-dev-12-1 (12.1.105-1) ...\n",
            "Setting up cuda-cudart-dev-12-1 (12.1.105-1) ...\n",
            "Setting up libcufft-12-1 (11.0.2.54-1) ...\n",
            "Setting up cuda-nvcc-12-1 (12.1.105-1) ...\n",
            "Setting up libcublas-12-1 (12.1.3.1-1) ...\n",
            "Setting up libnvjpeg-dev-12-1 (12.2.0.2-1) ...\n",
            "Setting up libcufile-dev-12-1 (1.6.1.9-1) ...\n",
            "Setting up libcufft-dev-12-1 (11.0.2.54-1) ...\n",
            "Setting up cuda-opencl-dev-12-1 (12.1.105-1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up cuda-command-line-tools-12-1 (12.1.1-1) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up cuda-compiler-12-1 (12.1.1-1) ...\n",
            "Setting up cuda-libraries-12-1 (12.1.1-1) ...\n",
            "Setting up libcublas-dev-12-1 (12.1.3.1-1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up cuda-libraries-dev-12-1 (12.1.1-1) ...\n",
            "Setting up cuda-visual-tools-12-1 (12.1.1-1) ...\n",
            "Setting up cuda-tools-12-1 (12.1.1-1) ...\n",
            "Setting up cuda-toolkit-12-1 (12.1.1-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3iQbVV9atsl",
        "outputId": "ae252d31-fe5d-4a29-fcde-cffdd105b512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.2)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes\n",
        "# Restart the runtime after installing to ensure the updated library is loaded\n",
        "# If you're using a Jupyter Notebook, you can restart the kernel.\n",
        "# In Colab, you can restart the runtime from the 'Runtime' menu.\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "# ... (rest of the code remains the same)\n",
        "\n",
        "torch_compile_options = {\n",
        "    \"epilogue_fusion\"   : True,\n",
        "    \"max_autotune\"      : True,\n",
        "    \"shape_padding\"     : True,\n",
        "    \"trace.enabled\"     : True,\n",
        "    \"triton.cudagraphs\" : False,\n",
        "}\n",
        "\n",
        "# Environment settings\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
        "\n",
        "torch.set_default_dtype(torch.float16)\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "\n",
        "# Load quantization config\n",
        "dtype = torch.float16\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit              = True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type       = \"nf4\",\n",
        "    bnb_4bit_compute_dtype    = dtype,\n",
        ")\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map = \"auto\",\n",
        "    attn_implementation = \"sdpa\",\n",
        "    quantization_config = bnb_config,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# LoRA Configuration\n",
        "lora_config = LoraConfig(\n",
        "    r = 32,\n",
        "    lora_alpha = 64,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    task_type = TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Get LoRA and setup model\n",
        "model = get_peft_model(model, lora_config)\n",
        "with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "        if \".lora_A.\" in name or \".lora_B.\" in name:\n",
        "            param.requires_grad_(True)\n",
        "        else:\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "# Enable input gradients\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# Detect graph breaks\n",
        "def detect_graph_breaks():\n",
        "    with torch.no_grad():\n",
        "        x = torch.randint(0, tokenizer.vocab_size, (1, 32), device=model.device, dtype=torch.long)  # Use torch.randint and dtype=torch.long\n",
        "        compiled_fn = torch.compile(model.forward, fullgraph=False, dynamic=True, options=torch_compile_options)\n",
        "        compiled_fn(input_ids=x) # Pass x as input_ids\n",
        "\n",
        "detect_graph_breaks()\n",
        "\n",
        "# Compile attention and MLP modules\n",
        "import transformers.models.llama.modeling_llama as llama_mod\n",
        "@torch.compile(fullgraph=False, dynamic=True, options=torch_compile_options)\n",
        "def compiled_llama_mlp(self, x):\n",
        "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "    return down_proj\n",
        "\n",
        "llama_mod.LlamaMLP.forward = compiled_llama_mlp\n",
        "\n",
        "@torch.compile(fullgraph=False, dynamic=True, options=torch_compile_options)\n",
        "def compiled_llama_attention(self, hidden_states, attention_mask=None):\n",
        "    return self.self_attn(hidden_states, attention_mask=attention_mask)\n",
        "\n",
        "llama_mod.LlamaAttention.forward = compiled_llama_attention\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": url}, split=\"train[:10%]\")\n",
        "\n",
        "print(\"Model compiled successfully without excessive graph breaks!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f9d3efb3eca843a49b6ebd3f31694a5c",
            "8c3c3b94822841d5b4c420cc99c6f8c7",
            "b305ba3d1e574bfb923b79b4b1dd885e",
            "18ea125d76a24b34b670636cb6de0ab3",
            "11b072dc036b42fcbe6305fe7c9f1782",
            "09daa87c07c146f6a0b7f3e20131d7ca",
            "beae36bd1a5d4781bf97d2cdaf7d65a8",
            "3d253120f9dc455f883aed0146aa688a",
            "5902ca74e65741479916cab985d2a1b8",
            "df9b0175c88d4df29aec19934c4d923b",
            "9ce2e3165da74d47a16244c636a84f58",
            "eb30a1d2d7ca4321a0a388e4583c9df7",
            "adc6327d05404093ba07a390060dac22",
            "02c9eef5c2f44a32b3832158f8a037d6",
            "53e0ca10ab644c5cb03126584a364e9d",
            "ca720202593b49839baea9592ea66b77",
            "a89a2c8b556342dc8c829ee790405334",
            "10a947ea5d13489baa085a3d9d9d515b",
            "d8a1044f06d24b0883ee7af4139239a3",
            "7f003d4d15f642549db84a33e0ae6ccc",
            "62b15076c5794693a4945cf125b8558f",
            "b0c92cf628a74108b02d6f6231d77d9b",
            "aae277d6c2414a47a58c746c4827ef9e",
            "0f22b90dacdf42ba92da453d970af0e0",
            "cc4df9dafa874a5badc6dad6987c794e",
            "891681f7a6fc45478ba247388143c598",
            "025d6114703c41ee956113473424e3ae",
            "f91dd130133648da9e9f221e3d66b045",
            "9135352006e14526b5dcc3c6a24fd177",
            "e52d28fe4de64a3a8ccfc8a987c0711a",
            "4039742297f248eea1cc9cb69c8c8ab7",
            "bfaeaa3ccaed45b3bb364ed0b3b794b9",
            "142de0e017e644cb83744b10bcd48649",
            "bb5bbf18e83948d794fdaa530f529d39",
            "a1530b354f344b7387e5df8c4467c208",
            "546920513edf4402beda96c3a7db80d7",
            "ae7f6250e6de4859adbe05a1bc7f20af",
            "4c7ffc4ba13448589a2a22226fe2a925",
            "885c0432ccf94d7a97bc22310a58a624",
            "fcaf3cbe96f24e15be8446d39f4dd7f5",
            "cdc833e684b448199a3b9bb297c3c397",
            "7aa5a27f2e9f4ff3afa8e4f4a11b07e8",
            "48a2c87144d148eb9d10eeec101293ec",
            "89821fa693da461c823adb73ca09f937",
            "df4d2580bb5c4bce865d12cf4986bf3c",
            "6bf7adf5027242a59295324135cd2e20",
            "3f696ee8157840039c3fbcf32c2dfed2",
            "5d5187a290be4a8e86a41e5eb696b134",
            "3fafbd292bb140bdbc867148590e2a00",
            "4c7b7f05aaa6403c807416bae3ada0c4",
            "1ed9fce664474886982e44afe665f1f1",
            "a172f94b674a4646b34b049959c786b3",
            "2efe7116dd2a425da06982e74656ace1",
            "40ddacea4da14b8394549c91345e0656",
            "cb9b2058317c416196d2839d774fcd31",
            "81c079cec188459a9494affcaff64f6e",
            "14dff2a1a73a4555ae569137ed5d5402",
            "920015a3491c426ca9ecc988bbafbebe",
            "7552117f013f41e3b70ab8640074e2ef",
            "c74056539b1f4069a79606f6bcb0f975",
            "97f8eadf79aa4013a66032447a715391",
            "06913f4ae319440ea48b90e919822ff6",
            "77fbf69e377e43248dd300f503863e78",
            "8a0c220aa7cd4b799102af803d09025b",
            "3966043a52544e79a52bcc69cdf1f8ce",
            "e610e267ce374e319184427e347f69f5",
            "bb9b013fdc144e2f8f4dae54b9be75cc",
            "f5a8892845f14c27842da3728c8d1e2b",
            "33942bc64379494a9e1dc475ba8f7154",
            "48d3efae8418461282ccfa6d2dd61821",
            "1b75df199f0242b8886e06c2501ac7b8",
            "1fbf42d82f594063966921d2c418978b",
            "5d076c03989d4142a9d8ae3c5c42accf",
            "8e4a2d181af1495f8a8508bca8c4c61e",
            "3324e4f359b047df945546cbe8bef79a",
            "acb7d15c65f047eb9163e7af82ce9af7",
            "6def145ab4c242bcb5bec49c990cf707",
            "d816f6351c3947509db0a9de661149af",
            "1a478e4dc6c84b92b3b24ba2ebf4d027",
            "3efa55ba7c1443b9b5da3e591ae5896b",
            "38e4318e322140cf9ff4220042e9b776",
            "594be35d0f3d4f47afa740ccb72212ad",
            "1f2517aaebfd41e29c12e482564dc018",
            "4eeddf5b5618459f88a0feb1149b6844",
            "85d6ea21dd124ec28b7da225b4178f32",
            "1ceedb95b3454cebb7df1eea855bc553",
            "fea4453e5d674b6ea2ea505853c9f085",
            "fc500718452a40609a416e6f249560a2"
          ]
        },
        "id": "vA-qAaS2ei5d",
        "outputId": "b50fcb16-2140-4d6d-b4b4-3ff11fa3d7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.2)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9d3efb3eca843a49b6ebd3f31694a5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:195: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb30a1d2d7ca4321a0a388e4583c9df7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aae277d6c2414a47a58c746c4827ef9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb5bbf18e83948d794fdaa530f529d39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df4d2580bb5c4bce865d12cf4986bf3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81c079cec188459a9494affcaff64f6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:07:36.748000 798 torch/_inductor/debug.py:435] [13/0] model__0_inference_0 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__0_inference_0.0\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:07:37.156000 798 torch/_inductor/utils.py:1137] [14/0] Not enough SMs to use max_autotune_gemm mode\n",
            "W0219 00:07:39.333000 798 torch/_inductor/debug.py:435] [14/0] model__1_inference_1 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__1_inference_1.1\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:07:41.586000 798 torch/_inductor/debug.py:435] [15/0_1] model__2_inference_2 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__2_inference_2.2\n",
            "W0219 00:07:41.911000 798 torch/_inductor/debug.py:435] [16/0_1] model__3_inference_3 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__3_inference_3.3\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "W0219 00:07:42.138000 798 torch/_inductor/debug.py:435] [18/0_1] model__4_inference_4 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__4_inference_4.4\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin None._SimpleCData.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "  torch._dynamo.utils.warn_once(msg)\n",
            "W0219 00:07:42.343000 798 torch/_inductor/debug.py:435] [21/0_1] model__5_inference_5 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__5_inference_5.5\n",
            "W0219 00:07:42.451000 798 torch/_inductor/debug.py:435] [22/0_1] model__6_inference_6 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__6_inference_6.6\n",
            "W0219 00:07:42.587000 798 torch/_inductor/debug.py:435] [23/0_1] model__7_inference_7 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__7_inference_7.7\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:07:42.755000 798 torch/_inductor/debug.py:435] [24/0_1] model__8_inference_8 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__8_inference_8.8\n",
            "W0219 00:07:43.175000 798 torch/_inductor/debug.py:435] [30/0_1] model__9_inference_9 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__9_inference_9.9\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin None.CFuncPtr.__new__. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.\n",
            "  torch._dynamo.utils.warn_once(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "W0219 00:07:43.520000 798 torch/_inductor/debug.py:435] [35/0_1] model__10_inference_10 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__10_inference_10.10\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "W0219 00:07:43.802000 798 torch/_inductor/debug.py:435] [38/0] model__11_inference_11 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__11_inference_11.11\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:07:45.087000 798 torch/_inductor/debug.py:435] [39/0] model__12_inference_12 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__12_inference_12.12\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:07:46.109000 798 torch/_inductor/debug.py:435] [40/0_1] model__13_inference_13 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__13_inference_13.13\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:07:46.308000 798 torch/_inductor/debug.py:435] [24/1_1] model__14_inference_14 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__14_inference_14.14\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:07:46.738000 798 torch/_inductor/debug.py:435] [39/1] model__15_inference_15 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__15_inference_15.15\n",
            "W0219 00:07:47.038000 798 torch/_inductor/debug.py:435] [41/0_1] model__16_inference_16 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__16_inference_16.16\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:07:55.255000 798 torch/_inductor/debug.py:435] [42/0_1] model__17_inference_17 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__17_inference_17.17\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:07:57.796000 798 torch/_inductor/debug.py:435] [44/0_1] model__18_inference_18 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__18_inference_18.18\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:07:58.642000 798 torch/_inductor/debug.py:435] [39/2] model__19_inference_19 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__19_inference_19.19\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:07:59.277000 798 torch/_inductor/debug.py:435] [46/0_1] model__20_inference_20 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__20_inference_20.20\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:07:59.747000 798 torch/_inductor/debug.py:435] [47/0_1] model__21_inference_21 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__21_inference_21.21\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:08:00.368000 798 torch/_inductor/debug.py:435] [39/3] model__22_inference_22 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__22_inference_22.22\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:08:00.808000 798 torch/_inductor/debug.py:435] [49/0] model__23_inference_23 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__23_inference_23.23\n",
            "W0219 00:08:01.150000 798 torch/_inductor/debug.py:435] [15/1_1] model__24_inference_24 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__24_inference_24.24\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:08:05.591000 798 torch/_inductor/debug.py:435] [42/1_1] model__25_inference_25 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__25_inference_25.25\n",
            "W0219 00:08:06.805000 798 torch/_inductor/debug.py:435] [44/1_1] model__26_inference_26 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__26_inference_26.26\n",
            "W0219 00:08:08.913000 798 torch/_inductor/debug.py:435] [42/2_1] model__27_inference_27 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__27_inference_27.27\n",
            "W0219 00:08:11.359000 798 torch/_inductor/debug.py:435] [42/3_1] model__28_inference_28 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__28_inference_28.28\n",
            "W0219 00:08:14.414000 798 torch/_inductor/debug.py:435] [42/4_1] model__29_inference_29 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__29_inference_29.29\n",
            "W0219 00:08:17.308000 798 torch/_inductor/debug.py:435] [42/5_1] model__30_inference_30 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__30_inference_30.30\n",
            "W0219 00:08:20.053000 798 torch/_inductor/debug.py:435] [42/6_1] model__31_inference_31 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__31_inference_31.31\n",
            "W0219 00:08:22.478000 798 torch/_inductor/debug.py:435] [42/7_1] model__32_inference_32 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__32_inference_32.32\n",
            "W0219 00:08:22.936000 798 torch/_dynamo/convert_frame.py:906] [42/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "W0219 00:08:22.936000 798 torch/_dynamo/convert_frame.py:906] [42/8]    function: 'torch_dynamo_resume_in_forward_at_271' (/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:271)\n",
            "W0219 00:08:22.936000 798 torch/_dynamo/convert_frame.py:906] [42/8]    last reason: 42/0: not L['past_key_value'].key_cache                           \n",
            "W0219 00:08:22.936000 798 torch/_dynamo/convert_frame.py:906] [42/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "W0219 00:08:22.936000 798 torch/_dynamo/convert_frame.py:906] [42/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "W0219 00:08:23.269000 798 torch/_inductor/debug.py:435] [50/0] model__33_inference_33 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__33_inference_33.33\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/_config_module.py:342: UserWarning: Skipping serialization of skipfiles_inline_module_allowlist value {}\n",
            "  warnings.warn(\n",
            "W0219 00:08:23.673000 798 torch/_inductor/debug.py:435] [53/0] model__34_inference_34 debug trace: /content/torch_compile_debug/run_2025_02_19_00_07_30_586713-pid_798/torchinductor/model__34_inference_34.34\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unified_chip2.jsonl:   0%|          | 0.00/95.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb9b013fdc144e2f8f4dae54b9be75cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d816f6351c3947509db0a9de661149af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled successfully without excessive graph breaks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import AdamW\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a QLoRA model\n",
        "class QLoRAModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, rank):\n",
        "        super(QLoRAModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "        # LoRA Parameters (Fixed Syntax)\n",
        "        self.lora_A = nn.Parameter(torch.randn(input_dim, rank) * 0.01)  # [input_dim, rank]\n",
        "        self.lora_B = nn.Parameter(torch.randn(rank, output_dim) * 0.01)  # [rank, output_dim]\n",
        "\n",
        "        self.rank = rank\n",
        "\n",
        "    def forward(self, x):\n",
        "        base_output = self.linear(x)  # Shape: [batch_size, output_dim]\n",
        "        lora_update = torch.matmul(x, self.lora_A)  # [batch_size, input_dim] x [input_dim, rank] -> [batch_size, rank]\n",
        "        lora_update = torch.matmul(lora_update, self.lora_B)  # [batch_size, rank] x [rank, output_dim] -> [batch_size, output_dim]\n",
        "\n",
        "        return base_output + lora_update  # Shapes now match: [batch_size, output_dim]\n",
        "\n",
        "# Generate dummy data\n",
        "def generate_data(num_samples, input_dim):\n",
        "    x = torch.randn(num_samples, input_dim)\n",
        "    y = torch.randn(num_samples, 1)\n",
        "    return x, y\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, optimizer, scaler, num_epochs=10):\n",
        "    model.train()\n",
        "    total_time = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(dataloader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Use autocast for mixed precision training\n",
        "            #with autocast():\n",
        "            output = model(data)\n",
        "            loss = nn.functional.mse_loss(output, target)\n",
        "\n",
        "            # Scale loss and backpropagate\n",
        "            loss.backward() # Scale the loss and perform backward pass\n",
        "            #scaler.unscale_(optimizer) # Unscale the gradients before optimizer step\n",
        "            optimizer.step()\n",
        "            #scaler.update()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Log every 10 batches\n",
        "            if batch_idx % 10 == 0:\n",
        "                logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        total_time += epoch_time\n",
        "        logger.info(f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f} seconds, Avg Loss: {epoch_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    logger.info(f\"Total training time: {total_time:.2f} seconds\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    input_dim = 128\n",
        "    output_dim = 64\n",
        "    rank = 8\n",
        "    num_samples = 1000\n",
        "    batch_size = 32\n",
        "    num_epochs = 5\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    # Generate data\n",
        "    x, y = generate_data(num_samples, input_dim)\n",
        "    dataset = TensorDataset(x, y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize model, optimizer, and scaler\n",
        "    model = QLoRAModel(input_dim, output_dim, rank).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # Check if torch.compile is available\n",
        "    if hasattr(torch, 'compile'):\n",
        "        logger.info(\"Compiling the model with torch.compile...\")\n",
        "        model = torch.compile(model, mode='max-autotune')\n",
        "\n",
        "    # Log memory usage before training\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        start_memory = torch.cuda.memory_allocated()\n",
        "        logger.info(f\"Memory allocated before training: {start_memory / 1024 ** 2:.2f} MB\")\n",
        "\n",
        "    # Train the model\n",
        "    logger.info(\"Starting training...\")\n",
        "    train_model(model, dataloader, optimizer, scaler, num_epochs)\n",
        "\n",
        "    # Log memory usage after training\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        end_memory = torch.cuda.memory_allocated()\n",
        "        logger.info(f\"Memory allocated after training: {end_memory / 1024 ** 2:.2f} MB\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mCNIA5NfCpv",
        "outputId": "32bc53b3-6f68-44cc-fa84-c52f467df441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-7b2251ee4712>:97: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-24-7b2251ee4712>:58: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(output, target)\n",
            "<ipython-input-24-7b2251ee4712>:58: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(output, target)\n",
            "<ipython-input-24-7b2251ee4712>:58: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(output, target)\n",
            "AUTOTUNE addmm(8x64, 8x128, 128x64)\n",
            "  bias_addmm 0.0102 ms 100.0% \n",
            "  addmm 0.0150 ms 68.2% \n",
            "SingleProcess AUTOTUNE benchmarking takes 0.2274 seconds and 0.0004 seconds precompiling for 2 choices\n",
            "<ipython-input-24-7b2251ee4712>:58: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(output, target)\n",
            "<ipython-input-24-7b2251ee4712>:58: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(output, target)\n",
            "<ipython-input-24-7b2251ee4712>:58: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(output, target)\n",
            "<ipython-input-24-7b2251ee4712>:58: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(output, target)\n",
            "<ipython-input-24-7b2251ee4712>:58: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(output, target)\n",
            "<ipython-input-24-7b2251ee4712>:58: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(output, target)\n",
            "<ipython-input-24-7b2251ee4712>:58: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = nn.functional.mse_loss(output, target)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import AdamW\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a QLoRA model\n",
        "class QLoRAModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, rank):\n",
        "        super(QLoRAModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "        # LoRA Parameters (Fixed Syntax)\n",
        "        self.lora_A = nn.Parameter(torch.randn(input_dim, rank) * 0.01)\n",
        "        self.lora_B = nn.Parameter(torch.randn(rank, output_dim) * 0.01)\n",
        "\n",
        "        self.rank = rank\n",
        "\n",
        "    def forward(self, x):\n",
        "        base_output = self.linear(x)\n",
        "        lora_update = torch.matmul(x, self.lora_A)\n",
        "        lora_update = torch.matmul(lora_update, self.lora_B)\n",
        "        return base_output + lora_update\n",
        "\n",
        "# Generate dummy data\n",
        "def generate_data(num_samples, input_dim, output_dim):\n",
        "    x = torch.randn(num_samples, input_dim)\n",
        "    y = torch.randn(num_samples, output_dim)  # Ensure correct shape\n",
        "    return x, y\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    total_time = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(dataloader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "            loss = nn.functional.mse_loss(output, target)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Log every 10 batches\n",
        "            if batch_idx % 10 == 0:\n",
        "                logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        total_time += epoch_time\n",
        "        logger.info(f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f} seconds, Avg Loss: {epoch_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    logger.info(f\"Total training time: {total_time:.2f} seconds\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    input_dim = 128\n",
        "    output_dim = 64\n",
        "    rank = 8\n",
        "    num_samples = 1000\n",
        "    batch_size = 32\n",
        "    num_epochs = 5\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    # Generate data\n",
        "    x, y = generate_data(num_samples, input_dim, output_dim)\n",
        "    dataset = TensorDataset(x, y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = QLoRAModel(input_dim, output_dim, rank)\n",
        "\n",
        "    # ✅ Selective Compilation (Only Compile QLoRA)\n",
        "    if hasattr(torch, 'compile'):\n",
        "        logger.info(\"Compiling the model with torch.compile...\")\n",
        "        model.linear = torch.compile(model.linear, mode='reduce-overhead')  # Compile only the linear layer\n",
        "        # LoRA parameters are small, avoid unnecessary compilation\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Log memory usage before training\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        start_memory = torch.cuda.memory_allocated()\n",
        "        logger.info(f\"Memory allocated before training: {start_memory / 1024 ** 2:.2f} MB\")\n",
        "\n",
        "    # Train the model\n",
        "    logger.info(\"Starting training...\")\n",
        "    train_model(model, dataloader, optimizer, num_epochs)\n",
        "\n",
        "    # Log memory usage after training\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        end_memory = torch.cuda.memory_allocated()\n",
        "        logger.info(f\"Memory allocated after training: {end_memory / 1024 ** 2:.2f} MB\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "AUxa1wtYfF49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import AdamW\n",
        "import time\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a QLoRA model\n",
        "class QLoRAModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, rank):\n",
        "        super(QLoRAModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "        # LoRA Parameters (Fixed Syntax)\n",
        "        self.lora_A = nn.Parameter(torch.randn(input_dim, rank) * 0.01)\n",
        "        self.lora_B = nn.Parameter(torch.randn(rank, output_dim) * 0.01)\n",
        "\n",
        "        self.rank = rank\n",
        "\n",
        "    def forward(self, x):\n",
        "        base_output = self.linear(x)\n",
        "        lora_update = torch.matmul(x, self.lora_A)\n",
        "        lora_update = torch.matmul(lora_update, self.lora_B)\n",
        "        return base_output + lora_update\n",
        "\n",
        "# Generate dummy data\n",
        "def generate_data(num_samples, input_dim, output_dim):\n",
        "    x = torch.randn(num_samples, input_dim)\n",
        "    y = torch.randn(num_samples, output_dim)  # Ensure correct shape\n",
        "    return x, y\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    total_time = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(dataloader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "            loss = nn.functional.mse_loss(output, target)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Log every 10 batches\n",
        "            if batch_idx % 10 == 0:\n",
        "                logger.info(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        total_time += epoch_time\n",
        "        logger.info(f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f} seconds, Avg Loss: {epoch_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    logger.info(f\"Total training time: {total_time:.2f} seconds\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    input_dim = 128\n",
        "    output_dim = 64\n",
        "    rank = 8\n",
        "    num_samples = 1000\n",
        "    batch_size = 32\n",
        "    num_epochs = 5\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    # Generate data\n",
        "    x, y = generate_data(num_samples, input_dim, output_dim)\n",
        "    dataset = TensorDataset(x, y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = QLoRAModel(input_dim, output_dim, rank)\n",
        "\n",
        "    # ✅ Compile the entire model if possible\n",
        "    if hasattr(torch, 'compile'):\n",
        "        logger.info(\"Compiling the model with torch.compile...\")\n",
        "        model = torch.compile(model, mode='reduce-overhead')  # Compile the entire model\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Log memory usage before training\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        start_memory = torch.cuda.memory_allocated()\n",
        "        logger.info(f\"Memory allocated before training: {start_memory / 1024 ** 2:.2f} MB\")\n",
        "\n",
        "    # Train the model\n",
        "    logger.info(\"Starting training...\")\n",
        "    train_model(model, dataloader, optimizer, num_epochs)\n",
        "\n",
        "    # Log memory usage after training\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        end_memory = torch.cuda.memory_allocated()\n",
        "        logger.info(f\"Memory allocated after training: {end_memory / 1024 ** 2:.2f} MB\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6uDLNLyyNbK",
        "outputId": "02002ce5-332e-42dc-bcc3-4a47770b6f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0] torchdynamo start compiling forward <ipython-input-51-a990fb973d15>:28, stack (elided 5 frames):\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"<frozen runpy>\", line 88, in _run_code\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     ColabKernelApp.launch_instance()\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     app.start()\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     self.io_loop.start()\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     self.asyncio_loop.run_forever()\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     self._run_once()\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     handle._run()\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     self._context.run(self._callback, *self._args)\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     await self.process_one()\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     await dispatch(*args)\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     await result\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     reply_content = await reply_content\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     res = shell.run_cell(\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     return super().run_cell(*args, **kwargs)\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     result = self._run_cell(\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     return runner(coro)\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     coro.send(None)\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     if (await self.run_code(code, result,  async_=asy)):\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"<ipython-input-51-a990fb973d15>\", line 117, in <cell line: 0>\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     main()\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"<ipython-input-51-a990fb973d15>\", line 108, in main\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     train_model(model, dataloader, optimizer, num_epochs)\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"<ipython-input-51-a990fb973d15>\", line 54, in train_model\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     output = model(data)\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     return self._call_impl(*args, **kwargs)\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     return forward_call(*args, **kwargs)\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     return fn(*args, **kwargs)\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0]     return self._call_impl(*args, **kwargs)\n",
            "V0219 02:07:15.642000 798 torch/_dynamo/convert_frame.py:930] [0/0] \n",
            "I0219 02:07:15.644000 798 torch/_dynamo/symbolic_convert.py:2706] [0/0] Step 1: torchdynamo start tracing forward <ipython-input-51-a990fb973d15>:28\n",
            "I0219 02:07:15.645000 798 torch/fx/experimental/symbolic_shapes.py:3192] [0/0] create_env\n",
            "V0219 02:07:15.649000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source] TRACE starts_line <ipython-input-51-a990fb973d15>:28 in forward\n",
            "V0219 02:07:15.649000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source]         def forward(self, x):\n",
            "V0219 02:07:15.650000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE RESUME 0 []\n",
            "V0219 02:07:15.652000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source] TRACE starts_line <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:15.652000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source]             base_output = self.linear(x)\n",
            "V0219 02:07:15.653000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_FAST self []\n",
            "V0219 02:07:15.654000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_METHOD linear [LazyVariableTracker()]\n",
            "V0219 02:07:15.658000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_FAST x [NullVariable, UnspecializedBuiltinNNModuleVariable(Linear)]\n",
            "V0219 02:07:15.659000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE PRECALL 1 [NullVariable, UnspecializedBuiltinNNModuleVariable(Linear), LazyVariableTracker()]\n",
            "V0219 02:07:15.660000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE CALL 1 [NullVariable, UnspecializedBuiltinNNModuleVariable(Linear), LazyVariableTracker()]\n",
            "V0219 02:07:15.663000 798 torch/_dynamo/symbolic_convert.py:3159] [0/0] [__trace_call] TRACE inlined call forward from <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:15.663000 798 torch/_dynamo/symbolic_convert.py:3159] [0/0] [__trace_call]         base_output = self.linear(x)\n",
            "V0219 02:07:15.663000 798 torch/_dynamo/symbolic_convert.py:3159] [0/0] [__trace_call]                       ~~~~~~~~~~~^^^\n",
            "V0219 02:07:15.664000 798 torch/_dynamo/symbolic_convert.py:3160] [0/0] INLINING <code object forward at 0x7a184c24b330, file \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 124>, inlined according trace_rules.lookup MOD_INLINELIST\n",
            "V0219 02:07:15.665000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source] TRACE starts_line /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:124 in forward (Linear.forward) (inline depth: 1)\n",
            "V0219 02:07:15.665000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source]         def forward(self, input: Tensor) -> Tensor:\n",
            "V0219 02:07:15.667000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE RESUME 0 []\n",
            "V0219 02:07:15.668000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source] TRACE starts_line /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125 in forward (Linear.forward) (inline depth: 1)\n",
            "V0219 02:07:15.668000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source]             return F.linear(input, self.weight, self.bias)\n",
            "V0219 02:07:15.669000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_GLOBAL F []\n",
            "V0219 02:07:15.670000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_ATTR linear [NullVariable, PythonModuleVariable(<module 'torch.nn.functional' from '/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py'>)]\n",
            "V0219 02:07:15.671000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_FAST input [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>)]\n",
            "V0219 02:07:15.673000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_FAST self [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker()]\n",
            "V0219 02:07:15.675000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_ATTR weight [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), UnspecializedBuiltinNNModuleVariable(Linear)]\n",
            "V0219 02:07:15.677000 798 torch/_dynamo/variables/builder.py:2853] [0/0] wrap_to_fake L['self']._modules['linear']._parameters['weight'] (64, 128) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None, None], constraint_strides=[None, None], view_base_context=None, tensor_source=GetItemSource(base=UnspecializedBuiltinNNModuleSource(base=UnspecializedParamBufferSource(base=GetItemSource(base=UnspecializedNNModuleSource(base=AttrSource(base=LocalSource(local_name='self', is_input=True, is_derefed_cell_contents=False), member='_modules')), index='linear', index_is_slice=False), member='_parameters')), index='weight', index_is_slice=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.nn.parameter.Parameter'>\n",
            "V0219 02:07:15.682000 798 torch/_dynamo/output_graph.py:2156] [0/0] create_graph_input L_self_modules_linear_parameters_weight_ L['self']._modules['linear']._parameters['weight'] Parameter(FakeTensor(..., device='cuda:0', size=(64, 128), requires_grad=True)) at debug_level 0 before=False\n",
            "V0219 02:07:15.683000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_FAST self [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), TensorVariable()]\n",
            "V0219 02:07:15.684000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_ATTR bias [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), TensorVariable(), UnspecializedBuiltinNNModuleVariable(Linear)]\n",
            "V0219 02:07:15.686000 798 torch/_dynamo/variables/builder.py:2853] [0/0] wrap_to_fake L['self']._modules['linear']._parameters['bias'] (64,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], view_base_context=None, tensor_source=GetItemSource(base=UnspecializedBuiltinNNModuleSource(base=UnspecializedParamBufferSource(base=GetItemSource(base=UnspecializedNNModuleSource(base=AttrSource(base=LocalSource(local_name='self', is_input=True, is_derefed_cell_contents=False), member='_modules')), index='linear', index_is_slice=False), member='_parameters')), index='bias', index_is_slice=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.nn.parameter.Parameter'>\n",
            "V0219 02:07:15.689000 798 torch/_dynamo/output_graph.py:2156] [0/0] create_graph_input L_self_modules_linear_parameters_bias_ L['self']._modules['linear']._parameters['bias'] Parameter(FakeTensor(..., device='cuda:0', size=(64,), requires_grad=True)) at debug_level 0 before=False\n",
            "V0219 02:07:15.691000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE PRECALL 3 [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:15.692000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE CALL 3 [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:15.693000 798 torch/_dynamo/variables/builder.py:2853] [0/0] wrap_to_fake L['x'] (32, 128) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None, None], constraint_strides=[None, None], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\n",
            "V0219 02:07:15.697000 798 torch/_dynamo/output_graph.py:2156] [0/0] create_graph_input L_x_ L['x'] FakeTensor(..., device='cuda:0', size=(32, 128)) at debug_level 0 before=False\n",
            "V0219 02:07:15.698000 798 torch/_dynamo/output_graph.py:2017] [0/0] [__trace_call] TRACE FX call linear from /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125 in forward (Linear.forward) (inline depth: 1)\n",
            "V0219 02:07:15.698000 798 torch/_dynamo/output_graph.py:2017] [0/0] [__trace_call]         return F.linear(input, self.weight, self.bias)\n",
            "V0219 02:07:15.698000 798 torch/_dynamo/output_graph.py:2017] [0/0] [__trace_call]                ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0219 02:07:15.702000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE RETURN_VALUE None [TensorVariable()]\n",
            "V0219 02:07:15.703000 798 torch/_dynamo/symbolic_convert.py:3222] [0/0] DONE INLINING <code object forward at 0x7a184c24b330, file \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 124>\n",
            "V0219 02:07:15.704000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE STORE_FAST base_output [TensorVariable()]\n",
            "V0219 02:07:15.705000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source] TRACE starts_line <ipython-input-51-a990fb973d15>:30 in forward\n",
            "V0219 02:07:15.705000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source]             lora_update = torch.matmul(x, self.lora_A)\n",
            "V0219 02:07:15.706000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_GLOBAL torch []\n",
            "V0219 02:07:15.708000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_METHOD matmul [PythonModuleVariable(<module 'torch' from '/usr/local/lib/python3.11/dist-packages/torch/__init__.py'>)]\n",
            "V0219 02:07:15.709000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_FAST x [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>)]\n",
            "V0219 02:07:15.710000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_FAST self [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable()]\n",
            "V0219 02:07:15.711000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_ATTR lora_A [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), UnspecializedNNModuleVariable(QLoRAModel)]\n",
            "V0219 02:07:15.712000 798 torch/_dynamo/variables/builder.py:2853] [0/0] wrap_to_fake L['self']._parameters['lora_A'] (128, 8) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None, None], constraint_strides=[None, None], view_base_context=None, tensor_source=GetItemSource(base=UnspecializedNNModuleSource(base=UnspecializedParamBufferSource(base=LocalSource(local_name='self', is_input=True, is_derefed_cell_contents=False), member='_parameters')), index='lora_A', index_is_slice=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.nn.parameter.Parameter'>\n",
            "V0219 02:07:15.714000 798 torch/_dynamo/output_graph.py:2156] [0/0] create_graph_input L_self_parameters_lora_A_ L['self']._parameters['lora_A'] Parameter(FakeTensor(..., device='cuda:0', size=(128, 8), requires_grad=True)) at debug_level 0 before=False\n",
            "V0219 02:07:15.716000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE PRECALL 2 [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:15.717000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE CALL 2 [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:15.718000 798 torch/_dynamo/output_graph.py:2017] [0/0] [__trace_call] TRACE FX call matmul from <ipython-input-51-a990fb973d15>:30 in forward\n",
            "V0219 02:07:15.718000 798 torch/_dynamo/output_graph.py:2017] [0/0] [__trace_call]         lora_update = torch.matmul(x, self.lora_A)\n",
            "V0219 02:07:15.718000 798 torch/_dynamo/output_graph.py:2017] [0/0] [__trace_call]                       ~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
            "V0219 02:07:15.720000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE STORE_FAST lora_update [TensorVariable()]\n",
            "V0219 02:07:15.721000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source] TRACE starts_line <ipython-input-51-a990fb973d15>:31 in forward\n",
            "V0219 02:07:15.721000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source]             lora_update = torch.matmul(lora_update, self.lora_B)\n",
            "V0219 02:07:15.722000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_GLOBAL torch []\n",
            "V0219 02:07:15.723000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_METHOD matmul [PythonModuleVariable(<module 'torch' from '/usr/local/lib/python3.11/dist-packages/torch/__init__.py'>)]\n",
            "V0219 02:07:15.724000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_FAST lora_update [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>)]\n",
            "V0219 02:07:15.725000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_FAST self [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable()]\n",
            "V0219 02:07:15.726000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_ATTR lora_B [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), UnspecializedNNModuleVariable(QLoRAModel)]\n",
            "V0219 02:07:15.728000 798 torch/_dynamo/variables/builder.py:2853] [0/0] wrap_to_fake L['self']._parameters['lora_B'] (8, 64) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None, None], constraint_strides=[None, None], view_base_context=None, tensor_source=GetItemSource(base=UnspecializedNNModuleSource(base=UnspecializedParamBufferSource(base=LocalSource(local_name='self', is_input=True, is_derefed_cell_contents=False), member='_parameters')), index='lora_B', index_is_slice=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.nn.parameter.Parameter'>\n",
            "V0219 02:07:15.730000 798 torch/_dynamo/output_graph.py:2156] [0/0] create_graph_input L_self_parameters_lora_B_ L['self']._parameters['lora_B'] Parameter(FakeTensor(..., device='cuda:0', size=(8, 64), requires_grad=True)) at debug_level 0 before=False\n",
            "V0219 02:07:15.731000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE PRECALL 2 [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:15.732000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE CALL 2 [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:15.734000 798 torch/_dynamo/output_graph.py:2017] [0/0] [__trace_call] TRACE FX call matmul_1 from <ipython-input-51-a990fb973d15>:31 in forward\n",
            "V0219 02:07:15.734000 798 torch/_dynamo/output_graph.py:2017] [0/0] [__trace_call]         lora_update = torch.matmul(lora_update, self.lora_B)\n",
            "V0219 02:07:15.734000 798 torch/_dynamo/output_graph.py:2017] [0/0] [__trace_call]                       ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0219 02:07:15.736000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE STORE_FAST lora_update [TensorVariable()]\n",
            "V0219 02:07:15.737000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source] TRACE starts_line <ipython-input-51-a990fb973d15>:32 in forward\n",
            "V0219 02:07:15.737000 798 torch/_dynamo/symbolic_convert.py:932] [0/0] [__trace_source]             return base_output + lora_update\n",
            "V0219 02:07:15.738000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_FAST base_output []\n",
            "V0219 02:07:15.739000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE LOAD_FAST lora_update [TensorVariable()]\n",
            "V0219 02:07:15.739000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE BINARY_OP 0 [TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:15.741000 798 torch/_dynamo/output_graph.py:2017] [0/0] [__trace_call] TRACE FX call add from <ipython-input-51-a990fb973d15>:32 in forward\n",
            "V0219 02:07:15.741000 798 torch/_dynamo/output_graph.py:2017] [0/0] [__trace_call]         return base_output + lora_update\n",
            "V0219 02:07:15.741000 798 torch/_dynamo/output_graph.py:2017] [0/0] [__trace_call]                ~~~~~~~~~~~~^~~~~~~~~~~~~\n",
            "V0219 02:07:15.743000 798 torch/_dynamo/symbolic_convert.py:955] [0/0] [__trace_bytecode] TRACE RETURN_VALUE None [TensorVariable()]\n",
            "I0219 02:07:15.744000 798 torch/_dynamo/symbolic_convert.py:3028] [0/0] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
            "V0219 02:07:15.744000 798 torch/_dynamo/symbolic_convert.py:3032] [0/0] RETURN_VALUE triggered compile\n",
            "V0219 02:07:15.745000 798 torch/_dynamo/output_graph.py:972] [0/0] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file <ipython-input-51-a990fb973d15>, line 32 in forward>], graph_break=False)\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code] TRACED GRAPH\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]  ===== __compiled_fn_257 =====\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]  /usr/local/lib/python3.11/dist-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]     def forward(self, L_self_modules_linear_parameters_weight_: \"f16[64, 128][128, 1]cuda:0\", L_self_modules_linear_parameters_bias_: \"f16[64][1]cuda:0\", L_x_: \"f16[32, 128][128, 1]cuda:0\", L_self_parameters_lora_A_: \"f16[128, 8][8, 1]cuda:0\", L_self_parameters_lora_B_: \"f16[8, 64][64, 1]cuda:0\"):\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         l_self_modules_linear_parameters_weight_ = L_self_modules_linear_parameters_weight_\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         l_self_modules_linear_parameters_bias_ = L_self_modules_linear_parameters_bias_\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         l_x_ = L_x_\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         l_self_parameters_lora_a_ = L_self_parameters_lora_A_\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         l_self_parameters_lora_b_ = L_self_parameters_lora_B_\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         \n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]          # File: <ipython-input-51-a990fb973d15>:29 in forward, code: base_output = self.linear(x)\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         base_output: \"f16[32, 64][64, 1]cuda:0\" = torch._C._nn.linear(l_x_, l_self_modules_linear_parameters_weight_, l_self_modules_linear_parameters_bias_);  l_self_modules_linear_parameters_weight_ = l_self_modules_linear_parameters_bias_ = None\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         \n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]          # File: <ipython-input-51-a990fb973d15>:30 in forward, code: lora_update = torch.matmul(x, self.lora_A)\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         lora_update: \"f16[32, 8][8, 1]cuda:0\" = torch.matmul(l_x_, l_self_parameters_lora_a_);  l_x_ = l_self_parameters_lora_a_ = None\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         \n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]          # File: <ipython-input-51-a990fb973d15>:31 in forward, code: lora_update = torch.matmul(lora_update, self.lora_B)\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         lora_update_1: \"f16[32, 64][64, 1]cuda:0\" = torch.matmul(lora_update, l_self_parameters_lora_b_);  lora_update = l_self_parameters_lora_b_ = None\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         \n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]          # File: <ipython-input-51-a990fb973d15>:32 in forward, code: return base_output + lora_update\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         add: \"f16[32, 64][64, 1]cuda:0\" = base_output + lora_update_1;  base_output = lora_update_1 = None\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         return (add,)\n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code]         \n",
            "V0219 02:07:15.749000 798 torch/_dynamo/output_graph.py:1353] [0/0] [__graph_code] \n",
            "I0219 02:07:15.752000 798 torch/_dynamo/output_graph.py:1458] [0/0] Step 2: calling compiler function inductor\n",
            "I0219 02:07:15.959000 798 torch/fx/experimental/symbolic_shapes.py:4547] [0/0] produce_guards\n",
            "I0219 02:07:15.967000 798 torch/_dynamo/output_graph.py:1463] [0/0] Step 2: done compiler function inductor\n",
            "I0219 02:07:15.975000 798 torch/fx/experimental/symbolic_shapes.py:4547] [0/0] produce_guards\n",
            "V0219 02:07:15.976000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._modules['linear']._parameters['weight'].size()[0] 64 None\n",
            "V0219 02:07:15.977000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._modules['linear']._parameters['weight'].size()[1] 128 None\n",
            "V0219 02:07:15.978000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._modules['linear']._parameters['weight'].stride()[0] 128 None\n",
            "V0219 02:07:15.979000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._modules['linear']._parameters['weight'].stride()[1] 1 None\n",
            "V0219 02:07:15.980000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._modules['linear']._parameters['weight'].storage_offset() 0 None\n",
            "V0219 02:07:15.981000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._modules['linear']._parameters['bias'].size()[0] 64 None\n",
            "V0219 02:07:15.982000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._modules['linear']._parameters['bias'].stride()[0] 1 None\n",
            "V0219 02:07:15.983000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._modules['linear']._parameters['bias'].storage_offset() 0 None\n",
            "V0219 02:07:15.984000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['x'].size()[0] 32 None\n",
            "V0219 02:07:15.985000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['x'].size()[1] 128 None\n",
            "V0219 02:07:15.986000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['x'].stride()[0] 128 None\n",
            "V0219 02:07:15.987000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['x'].stride()[1] 1 None\n",
            "V0219 02:07:15.988000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['x'].storage_offset() 0 None\n",
            "V0219 02:07:15.989000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._parameters['lora_A'].size()[0] 128 None\n",
            "V0219 02:07:15.990000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._parameters['lora_A'].size()[1] 8 None\n",
            "V0219 02:07:15.991000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._parameters['lora_A'].stride()[0] 8 None\n",
            "V0219 02:07:15.991000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._parameters['lora_A'].stride()[1] 1 None\n",
            "V0219 02:07:15.992000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._parameters['lora_A'].storage_offset() 0 None\n",
            "V0219 02:07:15.993000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._parameters['lora_B'].size()[0] 8 None\n",
            "V0219 02:07:15.994000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._parameters['lora_B'].size()[1] 64 None\n",
            "V0219 02:07:15.995000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._parameters['lora_B'].stride()[0] 64 None\n",
            "V0219 02:07:15.996000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._parameters['lora_B'].stride()[1] 1 None\n",
            "V0219 02:07:15.997000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/0] track_symint L['self']._parameters['lora_B'].storage_offset() 0 None\n",
            "V0219 02:07:15.998000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._modules['linear']._parameters['weight'].size()[0] == 64\n",
            "V0219 02:07:15.999000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._modules['linear']._parameters['weight'].size()[1] == 128\n",
            "V0219 02:07:16.000000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._modules['linear']._parameters['weight'].stride()[0] == 128\n",
            "V0219 02:07:16.001000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._modules['linear']._parameters['weight'].stride()[1] == 1\n",
            "V0219 02:07:16.002000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._modules['linear']._parameters['weight'].storage_offset() == 0\n",
            "V0219 02:07:16.003000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._modules['linear']._parameters['bias'].size()[0] == 64\n",
            "V0219 02:07:16.004000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._modules['linear']._parameters['bias'].stride()[0] == 1\n",
            "V0219 02:07:16.005000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._modules['linear']._parameters['bias'].storage_offset() == 0\n",
            "V0219 02:07:16.006000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['x'].size()[0] == 32\n",
            "V0219 02:07:16.007000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['x'].size()[1] == 128\n",
            "V0219 02:07:16.008000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['x'].stride()[0] == 128\n",
            "V0219 02:07:16.009000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['x'].stride()[1] == 1\n",
            "V0219 02:07:16.010000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['x'].storage_offset() == 0\n",
            "V0219 02:07:16.011000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._parameters['lora_A'].size()[0] == 128\n",
            "V0219 02:07:16.012000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._parameters['lora_A'].size()[1] == 8\n",
            "V0219 02:07:16.013000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._parameters['lora_A'].stride()[0] == 8\n",
            "V0219 02:07:16.014000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._parameters['lora_A'].stride()[1] == 1\n",
            "V0219 02:07:16.015000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._parameters['lora_A'].storage_offset() == 0\n",
            "V0219 02:07:16.016000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._parameters['lora_B'].size()[0] == 8\n",
            "V0219 02:07:16.016000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._parameters['lora_B'].size()[1] == 64\n",
            "V0219 02:07:16.018000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._parameters['lora_B'].stride()[0] == 64\n",
            "V0219 02:07:16.018000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._parameters['lora_B'].stride()[1] == 1\n",
            "V0219 02:07:16.019000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/0] Skipping guard L['self']._parameters['lora_B'].storage_offset() == 0\n",
            "V0219 02:07:16.025000 798 torch/_dynamo/guards.py:2364] [0/0] [__guards] GUARDS:\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] \n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] TREE_GUARD_MANAGER:\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] +- RootGuardManager\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:493 in init_ambient_guards\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | +- GuardManager: source=L['x'], accessed_by=DictGetItemGuardAccessor('x')\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=False, size=[32, 128], stride=[128, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor('self')\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | +- TYPE_MATCH: ___check_type_id(L['self'], 792671184)                        # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor('_modules')\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | +- DICT_LENGTH: len(L['self']._modules) == 1                                  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | +- GuardManager: source=L['self']._modules['linear'], accessed_by=DictGetItemGuardAccessor('linear')\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['linear'], 126011600)     # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | | +- GuardManager: source=L['self']._modules['linear'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['linear'].__dict__)  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | | | +- GuardManager: source=L['self']._modules['linear']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | | | | +- DICT_LENGTH: len(L['self']._modules['linear']._parameters) == 2            # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | | | | +- GuardManager: source=L['self']._modules['linear']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['linear']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=True, size=[64, 128], stride=[128, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | | | | +- GuardManager: source=L['self']._modules['linear']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['linear']._parameters['bias'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=True, size=[64], stride=[1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | +- GuardManager: source=L['self']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | +- DICT_LENGTH: len(L['self']._parameters) == 2                               # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | +- GuardManager: source=L['self']._parameters['lora_A'], accessed_by=DictGetItemGuardAccessor('lora_A')\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | | +- TENSOR_MATCH: check_tensor(L['self']._parameters['lora_A'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=True, size=[128, 8], stride=[8, 1])  # lora_update = torch.matmul(x, self.lora_A)  # <ipython-input-51-a990fb973d15>:30 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | +- GuardManager: source=L['self']._parameters['lora_B'], accessed_by=DictGetItemGuardAccessor('lora_B')\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | | +- TENSOR_MATCH: check_tensor(L['self']._parameters['lora_B'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=True, size=[8, 64], stride=[64, 1])  # lora_update = torch.matmul(lora_update, self.lora_B)  # <ipython-input-51-a990fb973d15>:31 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch')\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['torch'], 134245712056208)                  # lora_update = torch.matmul(x, self.lora_A)  # <ipython-input-51-a990fb973d15>:30 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | +- GuardManager: source=G['torch'].matmul, accessed_by=GetAttrGuardAccessor(matmul)\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].matmul, 134245275791616)           # lora_update = torch.matmul(x, self.lora_A)  # <ipython-input-51-a990fb973d15>:30 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_linear')\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'], 134244774415152)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F, accessed_by=GetAttrGuardAccessor(F)\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F, 134244774417392)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, accessed_by=GetAttrGuardAccessor(linear)\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, 134245244646432)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_module')\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'], 134244777627776)  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_hooks)\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_hooks)\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_pre_hooks)\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_pre_hooks)\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:16.027000 798 torch/_dynamo/guards.py:2321] [0/0] [__guards] \n",
            "V0219 02:07:17.030000 798 torch/_dynamo/guards.py:2346] [0/0] [__guards] Guard eval latency = 1.78 us\n",
            "I0219 02:07:17.031000 798 torch/_dynamo/pgo.py:636] [0/0] put_code_state: no cache key, skipping\n",
            "I0219 02:07:17.918000 798 torch/fx/experimental/symbolic_shapes.py:4547] [0/0] produce_guards\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1] torchdynamo start compiling forward <ipython-input-51-a990fb973d15>:28, stack (elided 5 frames):\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"<frozen runpy>\", line 88, in _run_code\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     ColabKernelApp.launch_instance()\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     app.start()\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     self.io_loop.start()\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     self.asyncio_loop.run_forever()\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     self._run_once()\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     handle._run()\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     self._context.run(self._callback, *self._args)\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     await self.process_one()\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     await dispatch(*args)\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     await result\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     reply_content = await reply_content\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     res = shell.run_cell(\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     return super().run_cell(*args, **kwargs)\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     result = self._run_cell(\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     return runner(coro)\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     coro.send(None)\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     if (await self.run_code(code, result,  async_=asy)):\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"<ipython-input-51-a990fb973d15>\", line 117, in <cell line: 0>\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     main()\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"<ipython-input-51-a990fb973d15>\", line 108, in main\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     train_model(model, dataloader, optimizer, num_epochs)\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"<ipython-input-51-a990fb973d15>\", line 54, in train_model\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     output = model(data)\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     return self._call_impl(*args, **kwargs)\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     return forward_call(*args, **kwargs)\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     return fn(*args, **kwargs)\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1]     return self._call_impl(*args, **kwargs)\n",
            "V0219 02:07:19.573000 798 torch/_dynamo/convert_frame.py:930] [0/1] \n",
            "I0219 02:07:19.577000 798 torch/_dynamo/symbolic_convert.py:2706] [0/1] Step 1: torchdynamo start tracing forward <ipython-input-51-a990fb973d15>:28\n",
            "I0219 02:07:19.578000 798 torch/fx/experimental/symbolic_shapes.py:3192] [0/1] create_env\n",
            "V0219 02:07:19.581000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source] TRACE starts_line <ipython-input-51-a990fb973d15>:28 in forward\n",
            "V0219 02:07:19.581000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source]         def forward(self, x):\n",
            "V0219 02:07:19.582000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE RESUME 0 []\n",
            "V0219 02:07:19.583000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source] TRACE starts_line <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:19.583000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source]             base_output = self.linear(x)\n",
            "V0219 02:07:19.584000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_FAST self []\n",
            "V0219 02:07:19.585000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_METHOD linear [LazyVariableTracker()]\n",
            "V0219 02:07:19.588000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_FAST x [NullVariable, UnspecializedBuiltinNNModuleVariable(Linear)]\n",
            "V0219 02:07:19.589000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE PRECALL 1 [NullVariable, UnspecializedBuiltinNNModuleVariable(Linear), LazyVariableTracker()]\n",
            "V0219 02:07:19.590000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE CALL 1 [NullVariable, UnspecializedBuiltinNNModuleVariable(Linear), LazyVariableTracker()]\n",
            "V0219 02:07:19.593000 798 torch/_dynamo/symbolic_convert.py:3159] [0/1] [__trace_call] TRACE inlined call forward from <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:19.593000 798 torch/_dynamo/symbolic_convert.py:3159] [0/1] [__trace_call]         base_output = self.linear(x)\n",
            "V0219 02:07:19.593000 798 torch/_dynamo/symbolic_convert.py:3159] [0/1] [__trace_call]                       ~~~~~~~~~~~^^^\n",
            "V0219 02:07:19.596000 798 torch/_dynamo/symbolic_convert.py:3160] [0/1] INLINING <code object forward at 0x7a184c24b330, file \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 124>, inlined according trace_rules.lookup MOD_INLINELIST\n",
            "V0219 02:07:19.598000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source] TRACE starts_line /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:124 in forward (Linear.forward) (inline depth: 1)\n",
            "V0219 02:07:19.598000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source]         def forward(self, input: Tensor) -> Tensor:\n",
            "V0219 02:07:19.599000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE RESUME 0 []\n",
            "V0219 02:07:19.601000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source] TRACE starts_line /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125 in forward (Linear.forward) (inline depth: 1)\n",
            "V0219 02:07:19.601000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source]             return F.linear(input, self.weight, self.bias)\n",
            "V0219 02:07:19.602000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_GLOBAL F []\n",
            "V0219 02:07:19.609000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_ATTR linear [NullVariable, PythonModuleVariable(<module 'torch.nn.functional' from '/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py'>)]\n",
            "V0219 02:07:19.619000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_FAST input [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>)]\n",
            "V0219 02:07:19.620000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_FAST self [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker()]\n",
            "V0219 02:07:19.622000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_ATTR weight [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), UnspecializedBuiltinNNModuleVariable(Linear)]\n",
            "V0219 02:07:19.629000 798 torch/_dynamo/variables/builder.py:2853] [0/1] wrap_to_fake L['self']._modules['linear']._parameters['weight'] (64, 128) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None, None], constraint_strides=[None, None], view_base_context=None, tensor_source=GetItemSource(base=UnspecializedBuiltinNNModuleSource(base=UnspecializedParamBufferSource(base=GetItemSource(base=UnspecializedNNModuleSource(base=AttrSource(base=LocalSource(local_name='self', is_input=True, is_derefed_cell_contents=False), member='_modules')), index='linear', index_is_slice=False), member='_parameters')), index='weight', index_is_slice=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.nn.parameter.Parameter'>\n",
            "V0219 02:07:19.633000 798 torch/_dynamo/output_graph.py:2156] [0/1] create_graph_input L_self_modules_linear_parameters_weight_ L['self']._modules['linear']._parameters['weight'] Parameter(FakeTensor(..., device='cuda:0', size=(64, 128), requires_grad=True)) at debug_level 0 before=False\n",
            "V0219 02:07:19.645000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_FAST self [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), TensorVariable()]\n",
            "V0219 02:07:19.657000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_ATTR bias [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), TensorVariable(), UnspecializedBuiltinNNModuleVariable(Linear)]\n",
            "V0219 02:07:19.659000 798 torch/_dynamo/variables/builder.py:2853] [0/1] wrap_to_fake L['self']._modules['linear']._parameters['bias'] (64,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], view_base_context=None, tensor_source=GetItemSource(base=UnspecializedBuiltinNNModuleSource(base=UnspecializedParamBufferSource(base=GetItemSource(base=UnspecializedNNModuleSource(base=AttrSource(base=LocalSource(local_name='self', is_input=True, is_derefed_cell_contents=False), member='_modules')), index='linear', index_is_slice=False), member='_parameters')), index='bias', index_is_slice=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.nn.parameter.Parameter'>\n",
            "V0219 02:07:19.662000 798 torch/_dynamo/output_graph.py:2156] [0/1] create_graph_input L_self_modules_linear_parameters_bias_ L['self']._modules['linear']._parameters['bias'] Parameter(FakeTensor(..., device='cuda:0', size=(64,), requires_grad=True)) at debug_level 0 before=False\n",
            "V0219 02:07:19.665000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE PRECALL 3 [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:19.666000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE CALL 3 [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:19.668000 798 torch/_dynamo/pgo.py:355] [0/1] automatic dynamic size L['x'] size(0) 8 != 32\n",
            "V0219 02:07:19.669000 798 torch/_dynamo/variables/builder.py:2853] [0/1] wrap_to_fake L['x'] (8, 128) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.DYNAMIC: 0>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[RelaxedUnspecConstraint(warn_only=True), None], constraint_strides=[None, None], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\n",
            "I0219 02:07:19.672000 798 torch/fx/experimental/symbolic_shapes.py:4423] [0/1] create_symbol s0 = 8 for L['x'].size()[0] [2, int_oo] return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward (_dynamo/variables/builder.py:2861 in <lambda>), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s0\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\"\n",
            "V0219 02:07:19.683000 798 torch/_dynamo/output_graph.py:2156] [0/1] create_graph_input L_x_ L['x'] FakeTensor(..., device='cuda:0', size=(s0, 128)) at debug_level 0 before=False\n",
            "V0219 02:07:19.684000 798 torch/_dynamo/output_graph.py:2156] [0/1] create_graph_input s0 L['x'].size()[0] s0 at debug_level 0 before=True\n",
            "V0219 02:07:19.686000 798 torch/_dynamo/output_graph.py:2482] [0/1] _lift_symbols_in_symint s0 from L['x'].size()[0] at debug_level 0\n",
            "V0219 02:07:19.698000 798 torch/_dynamo/output_graph.py:2017] [0/1] [__trace_call] TRACE FX call linear from /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125 in forward (Linear.forward) (inline depth: 1)\n",
            "V0219 02:07:19.698000 798 torch/_dynamo/output_graph.py:2017] [0/1] [__trace_call]         return F.linear(input, self.weight, self.bias)\n",
            "V0219 02:07:19.698000 798 torch/_dynamo/output_graph.py:2017] [0/1] [__trace_call]                ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0219 02:07:19.709000 798 torch/fx/experimental/symbolic_shapes.py:6412] [0/1] eval Eq(s0, 1) == False [statically known]\n",
            "V0219 02:07:19.752000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE RETURN_VALUE None [TensorVariable()]\n",
            "V0219 02:07:19.754000 798 torch/_dynamo/symbolic_convert.py:3222] [0/1] DONE INLINING <code object forward at 0x7a184c24b330, file \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 124>\n",
            "V0219 02:07:19.757000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE STORE_FAST base_output [TensorVariable()]\n",
            "V0219 02:07:19.759000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source] TRACE starts_line <ipython-input-51-a990fb973d15>:30 in forward\n",
            "V0219 02:07:19.759000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source]             lora_update = torch.matmul(x, self.lora_A)\n",
            "V0219 02:07:19.761000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_GLOBAL torch []\n",
            "V0219 02:07:19.762000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_METHOD matmul [PythonModuleVariable(<module 'torch' from '/usr/local/lib/python3.11/dist-packages/torch/__init__.py'>)]\n",
            "V0219 02:07:19.764000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_FAST x [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>)]\n",
            "V0219 02:07:19.765000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_FAST self [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable()]\n",
            "V0219 02:07:19.768000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_ATTR lora_A [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), UnspecializedNNModuleVariable(QLoRAModel)]\n",
            "V0219 02:07:19.770000 798 torch/_dynamo/variables/builder.py:2853] [0/1] wrap_to_fake L['self']._parameters['lora_A'] (128, 8) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None, None], constraint_strides=[None, None], view_base_context=None, tensor_source=GetItemSource(base=UnspecializedNNModuleSource(base=UnspecializedParamBufferSource(base=LocalSource(local_name='self', is_input=True, is_derefed_cell_contents=False), member='_parameters')), index='lora_A', index_is_slice=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.nn.parameter.Parameter'>\n",
            "V0219 02:07:19.775000 798 torch/_dynamo/output_graph.py:2156] [0/1] create_graph_input L_self_parameters_lora_A_ L['self']._parameters['lora_A'] Parameter(FakeTensor(..., device='cuda:0', size=(128, 8), requires_grad=True)) at debug_level 0 before=False\n",
            "V0219 02:07:19.778000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE PRECALL 2 [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:19.780000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE CALL 2 [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:19.781000 798 torch/_dynamo/output_graph.py:2017] [0/1] [__trace_call] TRACE FX call matmul from <ipython-input-51-a990fb973d15>:30 in forward\n",
            "V0219 02:07:19.781000 798 torch/_dynamo/output_graph.py:2017] [0/1] [__trace_call]         lora_update = torch.matmul(x, self.lora_A)\n",
            "V0219 02:07:19.781000 798 torch/_dynamo/output_graph.py:2017] [0/1] [__trace_call]                       ~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
            "V0219 02:07:19.787000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE STORE_FAST lora_update [TensorVariable()]\n",
            "V0219 02:07:19.788000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source] TRACE starts_line <ipython-input-51-a990fb973d15>:31 in forward\n",
            "V0219 02:07:19.788000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source]             lora_update = torch.matmul(lora_update, self.lora_B)\n",
            "V0219 02:07:19.789000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_GLOBAL torch []\n",
            "V0219 02:07:19.790000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_METHOD matmul [PythonModuleVariable(<module 'torch' from '/usr/local/lib/python3.11/dist-packages/torch/__init__.py'>)]\n",
            "V0219 02:07:19.791000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_FAST lora_update [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>)]\n",
            "V0219 02:07:19.792000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_FAST self [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable()]\n",
            "V0219 02:07:19.793000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_ATTR lora_B [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), UnspecializedNNModuleVariable(QLoRAModel)]\n",
            "V0219 02:07:19.808000 798 torch/_dynamo/variables/builder.py:2853] [0/1] wrap_to_fake L['self']._parameters['lora_B'] (8, 64) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None, None], constraint_strides=[None, None], view_base_context=None, tensor_source=GetItemSource(base=UnspecializedNNModuleSource(base=UnspecializedParamBufferSource(base=LocalSource(local_name='self', is_input=True, is_derefed_cell_contents=False), member='_parameters')), index='lora_B', index_is_slice=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.nn.parameter.Parameter'>\n",
            "V0219 02:07:19.813000 798 torch/_dynamo/output_graph.py:2156] [0/1] create_graph_input L_self_parameters_lora_B_ L['self']._parameters['lora_B'] Parameter(FakeTensor(..., device='cuda:0', size=(8, 64), requires_grad=True)) at debug_level 0 before=False\n",
            "V0219 02:07:19.829000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE PRECALL 2 [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:19.830000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE CALL 2 [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:19.836000 798 torch/_dynamo/output_graph.py:2017] [0/1] [__trace_call] TRACE FX call matmul_1 from <ipython-input-51-a990fb973d15>:31 in forward\n",
            "V0219 02:07:19.836000 798 torch/_dynamo/output_graph.py:2017] [0/1] [__trace_call]         lora_update = torch.matmul(lora_update, self.lora_B)\n",
            "V0219 02:07:19.836000 798 torch/_dynamo/output_graph.py:2017] [0/1] [__trace_call]                       ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0219 02:07:19.853000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE STORE_FAST lora_update [TensorVariable()]\n",
            "V0219 02:07:19.857000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source] TRACE starts_line <ipython-input-51-a990fb973d15>:32 in forward\n",
            "V0219 02:07:19.857000 798 torch/_dynamo/symbolic_convert.py:932] [0/1] [__trace_source]             return base_output + lora_update\n",
            "V0219 02:07:19.859000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_FAST base_output []\n",
            "V0219 02:07:19.860000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE LOAD_FAST lora_update [TensorVariable()]\n",
            "V0219 02:07:19.863000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE BINARY_OP 0 [TensorVariable(), TensorVariable()]\n",
            "V0219 02:07:19.868000 798 torch/_dynamo/output_graph.py:2017] [0/1] [__trace_call] TRACE FX call add from <ipython-input-51-a990fb973d15>:32 in forward\n",
            "V0219 02:07:19.868000 798 torch/_dynamo/output_graph.py:2017] [0/1] [__trace_call]         return base_output + lora_update\n",
            "V0219 02:07:19.868000 798 torch/_dynamo/output_graph.py:2017] [0/1] [__trace_call]                ~~~~~~~~~~~~^~~~~~~~~~~~~\n",
            "V0219 02:07:19.887000 798 torch/fx/experimental/symbolic_shapes.py:6412] [0/1] eval Ne(s0, 1) == True [statically known]\n",
            "V0219 02:07:19.898000 798 torch/_dynamo/symbolic_convert.py:955] [0/1] [__trace_bytecode] TRACE RETURN_VALUE None [TensorVariable()]\n",
            "I0219 02:07:19.900000 798 torch/_dynamo/symbolic_convert.py:3028] [0/1] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
            "V0219 02:07:19.902000 798 torch/_dynamo/symbolic_convert.py:3032] [0/1] RETURN_VALUE triggered compile\n",
            "V0219 02:07:19.903000 798 torch/_dynamo/output_graph.py:972] [0/1] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file <ipython-input-51-a990fb973d15>, line 32 in forward>], graph_break=False)\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code] TRACED GRAPH\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]  ===== __compiled_fn_259 =====\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]  /usr/local/lib/python3.11/dist-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]     def forward(self, L_self_modules_linear_parameters_weight_: \"f16[64, 128][128, 1]cuda:0\", L_self_modules_linear_parameters_bias_: \"f16[64][1]cuda:0\", s0: \"Sym(s0)\", L_x_: \"f16[s0, 128][128, 1]cuda:0\", L_self_parameters_lora_A_: \"f16[128, 8][8, 1]cuda:0\", L_self_parameters_lora_B_: \"f16[8, 64][64, 1]cuda:0\"):\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         l_self_modules_linear_parameters_weight_ = L_self_modules_linear_parameters_weight_\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         l_self_modules_linear_parameters_bias_ = L_self_modules_linear_parameters_bias_\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         l_x_ = L_x_\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         l_self_parameters_lora_a_ = L_self_parameters_lora_A_\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         l_self_parameters_lora_b_ = L_self_parameters_lora_B_\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         \n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]          # File: <ipython-input-51-a990fb973d15>:29 in forward, code: base_output = self.linear(x)\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         base_output: \"f16[s0, 64][64, 1]cuda:0\" = torch._C._nn.linear(l_x_, l_self_modules_linear_parameters_weight_, l_self_modules_linear_parameters_bias_);  l_self_modules_linear_parameters_weight_ = l_self_modules_linear_parameters_bias_ = None\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         \n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]          # File: <ipython-input-51-a990fb973d15>:30 in forward, code: lora_update = torch.matmul(x, self.lora_A)\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         lora_update: \"f16[s0, 8][8, 1]cuda:0\" = torch.matmul(l_x_, l_self_parameters_lora_a_);  l_x_ = l_self_parameters_lora_a_ = None\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         \n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]          # File: <ipython-input-51-a990fb973d15>:31 in forward, code: lora_update = torch.matmul(lora_update, self.lora_B)\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         lora_update_1: \"f16[s0, 64][64, 1]cuda:0\" = torch.matmul(lora_update, l_self_parameters_lora_b_);  lora_update = l_self_parameters_lora_b_ = None\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         \n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]          # File: <ipython-input-51-a990fb973d15>:32 in forward, code: return base_output + lora_update\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         add: \"f16[s0, 64][64, 1]cuda:0\" = base_output + lora_update_1;  base_output = lora_update_1 = None\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         return (add,)\n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code]         \n",
            "V0219 02:07:19.909000 798 torch/_dynamo/output_graph.py:1353] [0/1] [__graph_code] \n",
            "I0219 02:07:19.930000 798 torch/_dynamo/output_graph.py:1458] [0/1] Step 2: calling compiler function inductor\n",
            "I0219 02:07:20.908000 798 torch/fx/experimental/symbolic_shapes.py:4547] [0/1] produce_guards\n",
            "V0219 02:07:20.910000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['t0'] s0 None\n",
            "V0219 02:07:21.039000 798 torch/fx/experimental/symbolic_shapes.py:5802] [0/1] _update_var_to_range s0 = VR[2, 33554431] (update)\n",
            "I0219 02:07:21.042000 798 torch/fx/experimental/symbolic_shapes.py:6281] [0/1] eval s0 < 33554432 [guard added] (_inductor/codegen/simd.py:1245 in can_use_32bit_indexing), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"s0 < 33554432\"\n",
            "V0219 02:07:21.180000 798 torch/fx/experimental/symbolic_shapes.py:6412] [0/1] eval s0 < 2 == False [statically known]\n",
            "I0219 02:07:21.199000 798 torch/fx/experimental/symbolic_shapes.py:4547] [0/1] produce_guards\n",
            "V0219 02:07:21.208000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['t0'] s0 None\n",
            "I0219 02:07:21.223000 798 torch/_dynamo/output_graph.py:1463] [0/1] Step 2: done compiler function inductor\n",
            "I0219 02:07:21.244000 798 torch/fx/experimental/symbolic_shapes.py:4547] [0/1] produce_guards\n",
            "V0219 02:07:21.249000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._modules['linear']._parameters['weight'].size()[0] 64 None\n",
            "V0219 02:07:21.250000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._modules['linear']._parameters['weight'].size()[1] 128 None\n",
            "V0219 02:07:21.251000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._modules['linear']._parameters['weight'].stride()[0] 128 None\n",
            "V0219 02:07:21.252000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._modules['linear']._parameters['weight'].stride()[1] 1 None\n",
            "V0219 02:07:21.253000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._modules['linear']._parameters['weight'].storage_offset() 0 None\n",
            "V0219 02:07:21.254000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._modules['linear']._parameters['bias'].size()[0] 64 None\n",
            "V0219 02:07:21.255000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._modules['linear']._parameters['bias'].stride()[0] 1 None\n",
            "V0219 02:07:21.256000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._modules['linear']._parameters['bias'].storage_offset() 0 None\n",
            "V0219 02:07:21.259000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['x'].size()[0] s0 RelaxedUnspecConstraint(warn_only=True)\n",
            "V0219 02:07:21.260000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['x'].size()[1] 128 None\n",
            "V0219 02:07:21.261000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['x'].stride()[0] 128 None\n",
            "V0219 02:07:21.266000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['x'].stride()[1] 1 None\n",
            "V0219 02:07:21.267000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['x'].storage_offset() 0 None\n",
            "V0219 02:07:21.268000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._parameters['lora_A'].size()[0] 128 None\n",
            "V0219 02:07:21.270000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._parameters['lora_A'].size()[1] 8 None\n",
            "V0219 02:07:21.271000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._parameters['lora_A'].stride()[0] 8 None\n",
            "V0219 02:07:21.272000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._parameters['lora_A'].stride()[1] 1 None\n",
            "V0219 02:07:21.273000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._parameters['lora_A'].storage_offset() 0 None\n",
            "V0219 02:07:21.275000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._parameters['lora_B'].size()[0] 8 None\n",
            "V0219 02:07:21.277000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._parameters['lora_B'].size()[1] 64 None\n",
            "V0219 02:07:21.277000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._parameters['lora_B'].stride()[0] 64 None\n",
            "V0219 02:07:21.280000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._parameters['lora_B'].stride()[1] 1 None\n",
            "V0219 02:07:21.281000 798 torch/fx/experimental/symbolic_shapes.py:4755] [0/1] track_symint L['self']._parameters['lora_B'].storage_offset() 0 None\n",
            "V0219 02:07:21.283000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._modules['linear']._parameters['weight'].size()[0] == 64\n",
            "V0219 02:07:21.287000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._modules['linear']._parameters['weight'].size()[1] == 128\n",
            "V0219 02:07:21.288000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._modules['linear']._parameters['weight'].stride()[0] == 128\n",
            "V0219 02:07:21.291000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._modules['linear']._parameters['weight'].stride()[1] == 1\n",
            "V0219 02:07:21.292000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._modules['linear']._parameters['weight'].storage_offset() == 0\n",
            "V0219 02:07:21.293000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._modules['linear']._parameters['bias'].size()[0] == 64\n",
            "V0219 02:07:21.294000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._modules['linear']._parameters['bias'].stride()[0] == 1\n",
            "V0219 02:07:21.297000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._modules['linear']._parameters['bias'].storage_offset() == 0\n",
            "V0219 02:07:21.298000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['x'].size()[1] == 128\n",
            "V0219 02:07:21.299000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['x'].stride()[0] == 128\n",
            "V0219 02:07:21.300000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['x'].stride()[1] == 1\n",
            "V0219 02:07:21.303000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['x'].storage_offset() == 0\n",
            "V0219 02:07:21.304000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._parameters['lora_A'].size()[0] == 128\n",
            "V0219 02:07:21.305000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._parameters['lora_A'].size()[1] == 8\n",
            "V0219 02:07:21.306000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._parameters['lora_A'].stride()[0] == 8\n",
            "V0219 02:07:21.307000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._parameters['lora_A'].stride()[1] == 1\n",
            "V0219 02:07:21.309000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._parameters['lora_A'].storage_offset() == 0\n",
            "V0219 02:07:21.310000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._parameters['lora_B'].size()[0] == 8\n",
            "V0219 02:07:21.313000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._parameters['lora_B'].size()[1] == 64\n",
            "V0219 02:07:21.314000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._parameters['lora_B'].stride()[0] == 64\n",
            "V0219 02:07:21.315000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._parameters['lora_B'].stride()[1] == 1\n",
            "V0219 02:07:21.316000 798 torch/fx/experimental/symbolic_shapes.py:4958] [0/1] Skipping guard L['self']._parameters['lora_B'].storage_offset() == 0\n",
            "V0219 02:07:21.331000 798 torch/_dynamo/guards.py:2364] [0/1] [__guards] GUARDS:\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] \n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] TREE_GUARD_MANAGER:\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] +- RootGuardManager\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:493 in init_ambient_guards\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | +- GuardManager: source=L['x'], accessed_by=DictGetItemGuardAccessor('x')\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=False, size=[None, 128], stride=[128, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor('self')\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | +- TYPE_MATCH: ___check_type_id(L['self'], 792671184)                        # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor('_modules')\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | +- DICT_LENGTH: len(L['self']._modules) == 1                                  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | +- GuardManager: source=L['self']._modules['linear'], accessed_by=DictGetItemGuardAccessor('linear')\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['linear'], 126011600)     # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | | +- GuardManager: source=L['self']._modules['linear'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['linear'].__dict__)  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | | | +- GuardManager: source=L['self']._modules['linear']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | | | | +- DICT_LENGTH: len(L['self']._modules['linear']._parameters) == 2            # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | | | | +- GuardManager: source=L['self']._modules['linear']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['linear']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=True, size=[64, 128], stride=[128, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | | | | +- GuardManager: source=L['self']._modules['linear']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['linear']._parameters['bias'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=True, size=[64], stride=[1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | +- GuardManager: source=L['self']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | +- DICT_LENGTH: len(L['self']._parameters) == 2                               # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | +- GuardManager: source=L['self']._parameters['lora_A'], accessed_by=DictGetItemGuardAccessor('lora_A')\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | | +- TENSOR_MATCH: check_tensor(L['self']._parameters['lora_A'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=True, size=[128, 8], stride=[8, 1])  # lora_update = torch.matmul(x, self.lora_A)  # <ipython-input-51-a990fb973d15>:30 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | +- GuardManager: source=L['self']._parameters['lora_B'], accessed_by=DictGetItemGuardAccessor('lora_B')\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | | +- TENSOR_MATCH: check_tensor(L['self']._parameters['lora_B'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=True, size=[8, 64], stride=[64, 1])  # lora_update = torch.matmul(lora_update, self.lora_B)  # <ipython-input-51-a990fb973d15>:31 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch')\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['torch'], 134245712056208)                  # lora_update = torch.matmul(x, self.lora_A)  # <ipython-input-51-a990fb973d15>:30 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | +- GuardManager: source=G['torch'].matmul, accessed_by=GetAttrGuardAccessor(matmul)\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].matmul, 134245275791616)           # lora_update = torch.matmul(x, self.lora_A)  # <ipython-input-51-a990fb973d15>:30 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_linear')\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'], 134244774415152)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F, accessed_by=GetAttrGuardAccessor(F)\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F, 134244774417392)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, accessed_by=GetAttrGuardAccessor(linear)\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, 134245244646432)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_module')\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'], 134244777627776)  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_hooks)\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_hooks)\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_pre_hooks)\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_pre_hooks)\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks  # base_output = self.linear(x)  # <ipython-input-51-a990fb973d15>:29 in forward\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] +- LAMBDA_GUARD: 2 <= L['x'].size()[0]  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward (user code shown is first use of this value--the guard itself is not due user code but due to 0/1 specialization in the framework; to avoid specialization try torch._dynamo.mark_unbacked(tensor, dim))\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] +- LAMBDA_GUARD: L['x'].size()[0] <= 33554431  # (_inductor/codegen/simd.py:1245 in can_use_32bit_indexing)\n",
            "V0219 02:07:21.338000 798 torch/_dynamo/guards.py:2321] [0/1] [__guards] \n",
            "V0219 02:07:22.344000 798 torch/_dynamo/guards.py:2346] [0/1] [__guards] Guard eval latency = 11.84 us\n",
            "I0219 02:07:22.346000 798 torch/_dynamo/pgo.py:636] [0/1] put_code_state: no cache key, skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_model(model, dataloader, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    total_time = 0.0\n",
        "    memory_usage = []  # Store memory usage per epoch\n",
        "    epoch_times = []   # Store training time per epoch\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(dataloader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "            loss = nn.functional.mse_loss(output, target)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        total_time += epoch_time\n",
        "        epoch_times.append(epoch_time)\n",
        "\n",
        "        # Log VRAM usage per epoch\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "            current_memory = torch.cuda.memory_allocated() / 1024 ** 2  # Convert to MB\n",
        "            memory_usage.append(current_memory)\n",
        "\n",
        "        logger.info(f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f} sec, Avg Loss: {epoch_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    return memory_usage, epoch_times\n"
      ],
      "metadata": {
        "id": "BG4IXC-GyQLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(memory_usage, epoch_times, speedup):\n",
        "    epochs = range(1, len(memory_usage) + 1)\n",
        "\n",
        "    # Plot VRAM Usage Over Epochs\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, memory_usage, marker='o', linestyle='-', color='b', label=\"VRAM Usage\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Memory (MB)\")\n",
        "    plt.title(\"VRAM Usage Per Epoch\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    # Plot Speedup Comparison\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar([\"Without Compile\", \"With Compile\"], speedup, color=[\"red\", \"green\"])\n",
        "    plt.ylabel(\"Time (seconds)\")\n",
        "    plt.title(\"Speedup with torch.compile()\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    input_dim, output_dim, rank = 128, 64, 8\n",
        "    num_samples, batch_size, num_epochs = 1000, 32, 5\n",
        "    learning_rate = 1e-3\n",
        "\n",
        "    # Generate data\n",
        "    x, y = generate_data(num_samples, input_dim, output_dim)\n",
        "    dataset = TensorDataset(x, y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "   # Initialize model and optimizer\n",
        "    model = QLoRAModel(input_dim, output_dim, rank).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Measure speed without compilation\n",
        "    start_time = time.time()\n",
        "    memory_usage_no_compile, epoch_times_no_compile = train_model(model, dataloader, optimizer, num_epochs)\n",
        "    time_no_compile = time.time() - start_time\n",
        "\n",
        "    # ✅ Reinitialize optimizer for the compiled model\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Compile the model\n",
        "    if hasattr(torch, 'compile'):\n",
        "        logger.info(\"Compiling the model with torch.compile()...\")\n",
        "        model = torch.compile(model, mode='reduce-overhead')\n",
        "\n",
        "    # Measure speed with compilation\n",
        "    start_time = time.time()\n",
        "    memory_usage_compile, epoch_times_compile = train_model(model, dataloader, optimizer, num_epochs)\n",
        "    time_compile = time.time() - start_time\n",
        "\n",
        "    # ... (rest of your code) ...\n",
        "\n",
        "    # Compute speedup results\n",
        "    speedup = [time_no_compile, time_compile]\n",
        "\n",
        "    # Plot results\n",
        "    plot_results(memory_usage_compile, epoch_times_compile, speedup)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "r4_dMaT1yTnX",
        "outputId": "f7991047-7ff1-4302-9b0f-e8ad9f32d492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import logging\n",
        "import time\n",
        "\n",
        "# Enable logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Enable debug logging for graph breaks\n",
        "torch._logging.set_logs(dynamo=logging.INFO, inductor=logging.INFO)\n",
        "\n",
        "# Define a simple QLoRA model\n",
        "class QLoRAModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, rank):\n",
        "        super(QLoRAModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "        # LoRA Parameters\n",
        "        self.lora_A = nn.Parameter(torch.randn(input_dim, rank) * 0.01)\n",
        "        self.lora_B = nn.Parameter(torch.randn(rank, output_dim) * 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        base_output = self.linear(x)\n",
        "        lora_update = torch.matmul(x, self.lora_A)\n",
        "        lora_update = torch.matmul(lora_update, self.lora_B)\n",
        "        return base_output + lora_update\n",
        "\n",
        "# Function to detect graph breaks\n",
        "def check_graph_breaks(model, example_input):\n",
        "    \"\"\"\n",
        "    Runs torch.compile() and checks for graph breaks.\n",
        "    \"\"\"\n",
        "    logger.info(\"🔍 Checking for graph breaks...\")\n",
        "\n",
        "    # Get explanations for graph breaks\n",
        "    compiled_model = torch.compile(model, mode='reduce-overhead')\n",
        "\n",
        "    # Run a dummy forward pass\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            compiled_model(example_input)\n",
        "\n",
        "        # Check the number of graph breaks\n",
        "        num_graph_breaks = torch._dynamo.utils.counters[\"graph_break\"]\n",
        "\n",
        "        if num_graph_breaks > 0:\n",
        "            logger.warning(f\"🚨 GRAPH BREAK DETECTED! {num_graph_breaks} breaks occurred.\")\n",
        "            explanation = torch._dynamo.explain(model)(example_input)\n",
        "            logger.info(f\"🔍 Graph Break Explanation:\\n{explanation}\")\n",
        "        else:\n",
        "            logger.info(\"✅ No graph breaks detected! The model is fully compiled.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"⚠️ Error during compilation: {str(e)}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    input_dim, output_dim, rank = 128, 64, 8\n",
        "    model = QLoRAModel(input_dim, output_dim, rank).to(device)\n",
        "\n",
        "    # Generate dummy data\n",
        "    example_input = torch.randn(1, input_dim).to(device)\n",
        "\n",
        "    # Check for graph breaks\n",
        "    check_graph_breaks(model, example_input)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f4kEyZN_kiK",
        "outputId": "312828a0-bb35-4337-9633-da02369fb7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0219 02:23:07.457000 798 torch/_logging/_internal.py:440] Using TORCH_LOGS environment variable for log settings, ignoring call to set_logs\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0] torchdynamo start compiling forward <ipython-input-57-053a79d3adfa>:28, stack (elided 5 frames):\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"<frozen runpy>\", line 88, in _run_code\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     ColabKernelApp.launch_instance()\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     app.start()\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     self.io_loop.start()\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     self.asyncio_loop.run_forever()\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     self._run_once()\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     handle._run()\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     self._context.run(self._callback, *self._args)\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     await self.process_one()\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     await dispatch(*args)\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     await result\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     reply_content = await reply_content\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     res = shell.run_cell(\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     return super().run_cell(*args, **kwargs)\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     result = self._run_cell(\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     return runner(coro)\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     coro.send(None)\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     if (await self.run_code(code, result,  async_=asy)):\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"<ipython-input-57-053a79d3adfa>\", line 74, in <cell line: 0>\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     main()\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"<ipython-input-57-053a79d3adfa>\", line 71, in main\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     check_graph_breaks(model, example_input)\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"<ipython-input-57-053a79d3adfa>\", line 47, in check_graph_breaks\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     compiled_model(example_input)\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     return self._call_impl(*args, **kwargs)\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     return forward_call(*args, **kwargs)\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 574, in _fn\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     return fn(*args, **kwargs)\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0]     return self._call_impl(*args, **kwargs)\n",
            "V0219 02:23:07.467000 798 torch/_dynamo/convert_frame.py:930] [1/0] \n",
            "I0219 02:23:07.471000 798 torch/_dynamo/symbolic_convert.py:2706] [1/0] Step 1: torchdynamo start tracing forward <ipython-input-57-053a79d3adfa>:28\n",
            "I0219 02:23:07.474000 798 torch/fx/experimental/symbolic_shapes.py:3192] [1/0] create_env\n",
            "V0219 02:23:07.477000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source] TRACE starts_line <ipython-input-57-053a79d3adfa>:28 in forward\n",
            "V0219 02:23:07.477000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source]         def forward(self, x):\n",
            "V0219 02:23:07.479000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE RESUME 0 []\n",
            "V0219 02:23:07.480000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source] TRACE starts_line <ipython-input-57-053a79d3adfa>:29 in forward\n",
            "V0219 02:23:07.480000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source]             base_output = self.linear(x)\n",
            "V0219 02:23:07.481000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_FAST self []\n",
            "V0219 02:23:07.482000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_METHOD linear [LazyVariableTracker()]\n",
            "V0219 02:23:07.485000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_FAST x [NullVariable, UnspecializedBuiltinNNModuleVariable(Linear)]\n",
            "V0219 02:23:07.486000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE PRECALL 1 [NullVariable, UnspecializedBuiltinNNModuleVariable(Linear), LazyVariableTracker()]\n",
            "V0219 02:23:07.487000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE CALL 1 [NullVariable, UnspecializedBuiltinNNModuleVariable(Linear), LazyVariableTracker()]\n",
            "V0219 02:23:07.490000 798 torch/_dynamo/symbolic_convert.py:3159] [1/0] [__trace_call] TRACE inlined call forward from <ipython-input-57-053a79d3adfa>:29 in forward\n",
            "V0219 02:23:07.490000 798 torch/_dynamo/symbolic_convert.py:3159] [1/0] [__trace_call]         base_output = self.linear(x)\n",
            "V0219 02:23:07.490000 798 torch/_dynamo/symbolic_convert.py:3159] [1/0] [__trace_call]                       ~~~~~~~~~~~^^^\n",
            "V0219 02:23:07.491000 798 torch/_dynamo/symbolic_convert.py:3160] [1/0] INLINING <code object forward at 0x7a184c24b330, file \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 124>, inlined according trace_rules.lookup MOD_INLINELIST\n",
            "V0219 02:23:07.493000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source] TRACE starts_line /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:124 in forward (Linear.forward) (inline depth: 1)\n",
            "V0219 02:23:07.493000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source]         def forward(self, input: Tensor) -> Tensor:\n",
            "V0219 02:23:07.494000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE RESUME 0 []\n",
            "V0219 02:23:07.495000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source] TRACE starts_line /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125 in forward (Linear.forward) (inline depth: 1)\n",
            "V0219 02:23:07.495000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source]             return F.linear(input, self.weight, self.bias)\n",
            "V0219 02:23:07.496000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_GLOBAL F []\n",
            "V0219 02:23:07.498000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_ATTR linear [NullVariable, PythonModuleVariable(<module 'torch.nn.functional' from '/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py'>)]\n",
            "V0219 02:23:07.499000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_FAST input [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>)]\n",
            "V0219 02:23:07.500000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_FAST self [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker()]\n",
            "V0219 02:23:07.501000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_ATTR weight [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), UnspecializedBuiltinNNModuleVariable(Linear)]\n",
            "V0219 02:23:07.503000 798 torch/_dynamo/variables/builder.py:2853] [1/0] wrap_to_fake L['self']._modules['linear']._parameters['weight'] (64, 128) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None, None], constraint_strides=[None, None], view_base_context=None, tensor_source=GetItemSource(base=UnspecializedBuiltinNNModuleSource(base=UnspecializedParamBufferSource(base=GetItemSource(base=UnspecializedNNModuleSource(base=AttrSource(base=LocalSource(local_name='self', is_input=True, is_derefed_cell_contents=False), member='_modules')), index='linear', index_is_slice=False), member='_parameters')), index='weight', index_is_slice=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.nn.parameter.Parameter'>\n",
            "V0219 02:23:07.508000 798 torch/_dynamo/output_graph.py:2156] [1/0] create_graph_input L_self_modules_linear_parameters_weight_ L['self']._modules['linear']._parameters['weight'] Parameter(FakeTensor(..., device='cuda:0', size=(64, 128), requires_grad=True)) at debug_level 0 before=False\n",
            "V0219 02:23:07.510000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_FAST self [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), TensorVariable()]\n",
            "V0219 02:23:07.512000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_ATTR bias [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), TensorVariable(), UnspecializedBuiltinNNModuleVariable(Linear)]\n",
            "V0219 02:23:07.513000 798 torch/_dynamo/variables/builder.py:2853] [1/0] wrap_to_fake L['self']._modules['linear']._parameters['bias'] (64,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], view_base_context=None, tensor_source=GetItemSource(base=UnspecializedBuiltinNNModuleSource(base=UnspecializedParamBufferSource(base=GetItemSource(base=UnspecializedNNModuleSource(base=AttrSource(base=LocalSource(local_name='self', is_input=True, is_derefed_cell_contents=False), member='_modules')), index='linear', index_is_slice=False), member='_parameters')), index='bias', index_is_slice=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.nn.parameter.Parameter'>\n",
            "V0219 02:23:07.517000 798 torch/_dynamo/output_graph.py:2156] [1/0] create_graph_input L_self_modules_linear_parameters_bias_ L['self']._modules['linear']._parameters['bias'] Parameter(FakeTensor(..., device='cuda:0', size=(64,), requires_grad=True)) at debug_level 0 before=False\n",
            "V0219 02:23:07.519000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE PRECALL 3 [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), TensorVariable(), TensorVariable()]\n",
            "V0219 02:23:07.521000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE CALL 3 [NullVariable, TorchInGraphFunctionVariable(<built-in function linear>), LazyVariableTracker(), TensorVariable(), TensorVariable()]\n",
            "V0219 02:23:07.524000 798 torch/_dynamo/variables/builder.py:2853] [1/0] wrap_to_fake L['x'] (1, 128) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None, None], constraint_strides=[None, None], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\n",
            "V0219 02:23:07.527000 798 torch/_dynamo/output_graph.py:2156] [1/0] create_graph_input L_x_ L['x'] FakeTensor(..., device='cuda:0', size=(1, 128)) at debug_level 0 before=False\n",
            "V0219 02:23:07.530000 798 torch/_dynamo/output_graph.py:2017] [1/0] [__trace_call] TRACE FX call linear from /usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py:125 in forward (Linear.forward) (inline depth: 1)\n",
            "V0219 02:23:07.530000 798 torch/_dynamo/output_graph.py:2017] [1/0] [__trace_call]         return F.linear(input, self.weight, self.bias)\n",
            "V0219 02:23:07.530000 798 torch/_dynamo/output_graph.py:2017] [1/0] [__trace_call]                ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0219 02:23:07.559000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE RETURN_VALUE None [TensorVariable()]\n",
            "V0219 02:23:07.561000 798 torch/_dynamo/symbolic_convert.py:3222] [1/0] DONE INLINING <code object forward at 0x7a184c24b330, file \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 124>\n",
            "V0219 02:23:07.562000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE STORE_FAST base_output [TensorVariable()]\n",
            "V0219 02:23:07.563000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source] TRACE starts_line <ipython-input-57-053a79d3adfa>:30 in forward\n",
            "V0219 02:23:07.563000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source]             lora_update = torch.matmul(x, self.lora_A)\n",
            "V0219 02:23:07.565000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_GLOBAL torch []\n",
            "V0219 02:23:07.568000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_METHOD matmul [PythonModuleVariable(<module 'torch' from '/usr/local/lib/python3.11/dist-packages/torch/__init__.py'>)]\n",
            "V0219 02:23:07.569000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_FAST x [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>)]\n",
            "V0219 02:23:07.570000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_FAST self [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable()]\n",
            "V0219 02:23:07.575000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_ATTR lora_A [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), UnspecializedNNModuleVariable(QLoRAModel)]\n",
            "V0219 02:23:07.576000 798 torch/_dynamo/variables/builder.py:2853] [1/0] wrap_to_fake L['self']._parameters['lora_A'] (128, 8) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None, None], constraint_strides=[None, None], view_base_context=None, tensor_source=GetItemSource(base=UnspecializedNNModuleSource(base=UnspecializedParamBufferSource(base=LocalSource(local_name='self', is_input=True, is_derefed_cell_contents=False), member='_parameters')), index='lora_A', index_is_slice=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.nn.parameter.Parameter'>\n",
            "V0219 02:23:07.581000 798 torch/_dynamo/output_graph.py:2156] [1/0] create_graph_input L_self_parameters_lora_A_ L['self']._parameters['lora_A'] Parameter(FakeTensor(..., device='cuda:0', size=(128, 8), requires_grad=True)) at debug_level 0 before=False\n",
            "V0219 02:23:07.583000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE PRECALL 2 [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), TensorVariable()]\n",
            "V0219 02:23:07.586000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE CALL 2 [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), TensorVariable()]\n",
            "V0219 02:23:07.589000 798 torch/_dynamo/output_graph.py:2017] [1/0] [__trace_call] TRACE FX call matmul from <ipython-input-57-053a79d3adfa>:30 in forward\n",
            "V0219 02:23:07.589000 798 torch/_dynamo/output_graph.py:2017] [1/0] [__trace_call]         lora_update = torch.matmul(x, self.lora_A)\n",
            "V0219 02:23:07.589000 798 torch/_dynamo/output_graph.py:2017] [1/0] [__trace_call]                       ~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
            "V0219 02:23:07.594000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE STORE_FAST lora_update [TensorVariable()]\n",
            "V0219 02:23:07.597000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source] TRACE starts_line <ipython-input-57-053a79d3adfa>:31 in forward\n",
            "V0219 02:23:07.597000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source]             lora_update = torch.matmul(lora_update, self.lora_B)\n",
            "V0219 02:23:07.598000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_GLOBAL torch []\n",
            "V0219 02:23:07.602000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_METHOD matmul [PythonModuleVariable(<module 'torch' from '/usr/local/lib/python3.11/dist-packages/torch/__init__.py'>)]\n",
            "V0219 02:23:07.603000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_FAST lora_update [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>)]\n",
            "V0219 02:23:07.607000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_FAST self [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable()]\n",
            "V0219 02:23:07.608000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_ATTR lora_B [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), UnspecializedNNModuleVariable(QLoRAModel)]\n",
            "V0219 02:23:07.609000 798 torch/_dynamo/variables/builder.py:2853] [1/0] wrap_to_fake L['self']._parameters['lora_B'] (8, 64) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>, <DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None, None], constraint_strides=[None, None], view_base_context=None, tensor_source=GetItemSource(base=UnspecializedNNModuleSource(base=UnspecializedParamBufferSource(base=LocalSource(local_name='self', is_input=True, is_derefed_cell_contents=False), member='_parameters')), index='lora_B', index_is_slice=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.nn.parameter.Parameter'>\n",
            "V0219 02:23:07.616000 798 torch/_dynamo/output_graph.py:2156] [1/0] create_graph_input L_self_parameters_lora_B_ L['self']._parameters['lora_B'] Parameter(FakeTensor(..., device='cuda:0', size=(8, 64), requires_grad=True)) at debug_level 0 before=False\n",
            "V0219 02:23:07.617000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE PRECALL 2 [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), TensorVariable()]\n",
            "V0219 02:23:07.621000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE CALL 2 [NullVariable, TorchInGraphFunctionVariable(<built-in method matmul of type object at 0x7a183481ff00>), TensorVariable(), TensorVariable()]\n",
            "V0219 02:23:07.622000 798 torch/_dynamo/output_graph.py:2017] [1/0] [__trace_call] TRACE FX call matmul_1 from <ipython-input-57-053a79d3adfa>:31 in forward\n",
            "V0219 02:23:07.622000 798 torch/_dynamo/output_graph.py:2017] [1/0] [__trace_call]         lora_update = torch.matmul(lora_update, self.lora_B)\n",
            "V0219 02:23:07.622000 798 torch/_dynamo/output_graph.py:2017] [1/0] [__trace_call]                       ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0219 02:23:07.628000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE STORE_FAST lora_update [TensorVariable()]\n",
            "V0219 02:23:07.629000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source] TRACE starts_line <ipython-input-57-053a79d3adfa>:32 in forward\n",
            "V0219 02:23:07.629000 798 torch/_dynamo/symbolic_convert.py:932] [1/0] [__trace_source]             return base_output + lora_update\n",
            "V0219 02:23:07.632000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_FAST base_output []\n",
            "V0219 02:23:07.635000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE LOAD_FAST lora_update [TensorVariable()]\n",
            "V0219 02:23:07.636000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE BINARY_OP 0 [TensorVariable(), TensorVariable()]\n",
            "V0219 02:23:07.638000 798 torch/_dynamo/output_graph.py:2017] [1/0] [__trace_call] TRACE FX call add from <ipython-input-57-053a79d3adfa>:32 in forward\n",
            "V0219 02:23:07.638000 798 torch/_dynamo/output_graph.py:2017] [1/0] [__trace_call]         return base_output + lora_update\n",
            "V0219 02:23:07.638000 798 torch/_dynamo/output_graph.py:2017] [1/0] [__trace_call]                ~~~~~~~~~~~~^~~~~~~~~~~~~\n",
            "V0219 02:23:07.642000 798 torch/_dynamo/symbolic_convert.py:955] [1/0] [__trace_bytecode] TRACE RETURN_VALUE None [TensorVariable()]\n",
            "I0219 02:23:07.643000 798 torch/_dynamo/symbolic_convert.py:3028] [1/0] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
            "V0219 02:23:07.644000 798 torch/_dynamo/symbolic_convert.py:3032] [1/0] RETURN_VALUE triggered compile\n",
            "V0219 02:23:07.645000 798 torch/_dynamo/output_graph.py:972] [1/0] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file <ipython-input-57-053a79d3adfa>, line 32 in forward>], graph_break=False)\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code] TRACED GRAPH\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]  ===== __compiled_fn_261 =====\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]  /usr/local/lib/python3.11/dist-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]     def forward(self, L_self_modules_linear_parameters_weight_: \"f16[64, 128][128, 1]cuda:0\", L_self_modules_linear_parameters_bias_: \"f16[64][1]cuda:0\", L_x_: \"f16[1, 128][128, 1]cuda:0\", L_self_parameters_lora_A_: \"f16[128, 8][8, 1]cuda:0\", L_self_parameters_lora_B_: \"f16[8, 64][64, 1]cuda:0\"):\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         l_self_modules_linear_parameters_weight_ = L_self_modules_linear_parameters_weight_\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         l_self_modules_linear_parameters_bias_ = L_self_modules_linear_parameters_bias_\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         l_x_ = L_x_\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         l_self_parameters_lora_a_ = L_self_parameters_lora_A_\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         l_self_parameters_lora_b_ = L_self_parameters_lora_B_\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         \n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]          # File: <ipython-input-57-053a79d3adfa>:29 in forward, code: base_output = self.linear(x)\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         base_output: \"f16[1, 64][64, 1]cuda:0\" = torch._C._nn.linear(l_x_, l_self_modules_linear_parameters_weight_, l_self_modules_linear_parameters_bias_);  l_self_modules_linear_parameters_weight_ = l_self_modules_linear_parameters_bias_ = None\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         \n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]          # File: <ipython-input-57-053a79d3adfa>:30 in forward, code: lora_update = torch.matmul(x, self.lora_A)\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         lora_update: \"f16[1, 8][8, 1]cuda:0\" = torch.matmul(l_x_, l_self_parameters_lora_a_);  l_x_ = l_self_parameters_lora_a_ = None\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         \n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]          # File: <ipython-input-57-053a79d3adfa>:31 in forward, code: lora_update = torch.matmul(lora_update, self.lora_B)\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         lora_update_1: \"f16[1, 64][64, 1]cuda:0\" = torch.matmul(lora_update, l_self_parameters_lora_b_);  lora_update = l_self_parameters_lora_b_ = None\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         \n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]          # File: <ipython-input-57-053a79d3adfa>:32 in forward, code: return base_output + lora_update\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         add: \"f16[1, 64][64, 1]cuda:0\" = base_output + lora_update_1;  base_output = lora_update_1 = None\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         return (add,)\n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code]         \n",
            "V0219 02:23:07.649000 798 torch/_dynamo/output_graph.py:1353] [1/0] [__graph_code] \n",
            "I0219 02:23:07.653000 798 torch/_dynamo/output_graph.py:1458] [1/0] Step 2: calling compiler function inductor\n",
            "I0219 02:23:07.932000 798 torch/fx/experimental/symbolic_shapes.py:4547] [1/0] produce_guards\n",
            "I0219 02:23:07.944000 798 torch/_dynamo/output_graph.py:1463] [1/0] Step 2: done compiler function inductor\n",
            "I0219 02:23:07.956000 798 torch/fx/experimental/symbolic_shapes.py:4547] [1/0] produce_guards\n",
            "V0219 02:23:07.960000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._modules['linear']._parameters['weight'].size()[0] 64 None\n",
            "V0219 02:23:07.963000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._modules['linear']._parameters['weight'].size()[1] 128 None\n",
            "V0219 02:23:07.966000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._modules['linear']._parameters['weight'].stride()[0] 128 None\n",
            "V0219 02:23:07.968000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._modules['linear']._parameters['weight'].stride()[1] 1 None\n",
            "V0219 02:23:07.970000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._modules['linear']._parameters['weight'].storage_offset() 0 None\n",
            "V0219 02:23:07.972000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._modules['linear']._parameters['bias'].size()[0] 64 None\n",
            "V0219 02:23:07.974000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._modules['linear']._parameters['bias'].stride()[0] 1 None\n",
            "V0219 02:23:07.976000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._modules['linear']._parameters['bias'].storage_offset() 0 None\n",
            "V0219 02:23:07.978000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['x'].size()[0] 1 None\n",
            "V0219 02:23:07.980000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['x'].size()[1] 128 None\n",
            "V0219 02:23:07.982000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['x'].stride()[0] 128 None\n",
            "V0219 02:23:07.984000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['x'].stride()[1] 1 None\n",
            "V0219 02:23:07.986000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['x'].storage_offset() 0 None\n",
            "V0219 02:23:07.987000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._parameters['lora_A'].size()[0] 128 None\n",
            "V0219 02:23:07.988000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._parameters['lora_A'].size()[1] 8 None\n",
            "V0219 02:23:07.988000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._parameters['lora_A'].stride()[0] 8 None\n",
            "V0219 02:23:07.989000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._parameters['lora_A'].stride()[1] 1 None\n",
            "V0219 02:23:07.990000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._parameters['lora_A'].storage_offset() 0 None\n",
            "V0219 02:23:07.991000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._parameters['lora_B'].size()[0] 8 None\n",
            "V0219 02:23:07.992000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._parameters['lora_B'].size()[1] 64 None\n",
            "V0219 02:23:07.998000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._parameters['lora_B'].stride()[0] 64 None\n",
            "V0219 02:23:07.999000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._parameters['lora_B'].stride()[1] 1 None\n",
            "V0219 02:23:08.000000 798 torch/fx/experimental/symbolic_shapes.py:4755] [1/0] track_symint L['self']._parameters['lora_B'].storage_offset() 0 None\n",
            "V0219 02:23:08.002000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._modules['linear']._parameters['weight'].size()[0] == 64\n",
            "V0219 02:23:08.003000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._modules['linear']._parameters['weight'].size()[1] == 128\n",
            "V0219 02:23:08.004000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._modules['linear']._parameters['weight'].stride()[0] == 128\n",
            "V0219 02:23:08.005000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._modules['linear']._parameters['weight'].stride()[1] == 1\n",
            "V0219 02:23:08.010000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._modules['linear']._parameters['weight'].storage_offset() == 0\n",
            "V0219 02:23:08.011000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._modules['linear']._parameters['bias'].size()[0] == 64\n",
            "V0219 02:23:08.011000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._modules['linear']._parameters['bias'].stride()[0] == 1\n",
            "V0219 02:23:08.012000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._modules['linear']._parameters['bias'].storage_offset() == 0\n",
            "V0219 02:23:08.015000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['x'].size()[0] == 1\n",
            "V0219 02:23:08.016000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['x'].size()[1] == 128\n",
            "V0219 02:23:08.019000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['x'].stride()[0] == 128\n",
            "V0219 02:23:08.020000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['x'].stride()[1] == 1\n",
            "V0219 02:23:08.021000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['x'].storage_offset() == 0\n",
            "V0219 02:23:08.027000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._parameters['lora_A'].size()[0] == 128\n",
            "V0219 02:23:08.028000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._parameters['lora_A'].size()[1] == 8\n",
            "V0219 02:23:08.029000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._parameters['lora_A'].stride()[0] == 8\n",
            "V0219 02:23:08.030000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._parameters['lora_A'].stride()[1] == 1\n",
            "V0219 02:23:08.032000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._parameters['lora_A'].storage_offset() == 0\n",
            "V0219 02:23:08.033000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._parameters['lora_B'].size()[0] == 8\n",
            "V0219 02:23:08.036000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._parameters['lora_B'].size()[1] == 64\n",
            "V0219 02:23:08.037000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._parameters['lora_B'].stride()[0] == 64\n",
            "V0219 02:23:08.042000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._parameters['lora_B'].stride()[1] == 1\n",
            "V0219 02:23:08.043000 798 torch/fx/experimental/symbolic_shapes.py:4958] [1/0] Skipping guard L['self']._parameters['lora_B'].storage_offset() == 0\n",
            "V0219 02:23:08.056000 798 torch/_dynamo/guards.py:2364] [1/0] [__guards] GUARDS:\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] \n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] TREE_GUARD_MANAGER:\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] +- RootGuardManager\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:493 in init_ambient_guards\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | +- GLOBAL_STATE: ___check_global_state()\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | +- GuardManager: source=L['x'], accessed_by=DictGetItemGuardAccessor('x')\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=False, size=[1, 128], stride=[128, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | +- GuardManager: source=L['self'], accessed_by=DictGetItemGuardAccessor('self')\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | +- TYPE_MATCH: ___check_type_id(L['self'], 981313440)                        # base_output = self.linear(x)  # <ipython-input-57-053a79d3adfa>:29 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | +- GuardManager: source=L['self'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | +- GuardManager: source=L['self']._modules, accessed_by=DictGetItemGuardAccessor('_modules')\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | +- DICT_LENGTH: len(L['self']._modules) == 1                                  # base_output = self.linear(x)  # <ipython-input-57-053a79d3adfa>:29 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | +- GuardManager: source=L['self']._modules['linear'], accessed_by=DictGetItemGuardAccessor('linear')\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | | +- TYPE_MATCH: ___check_type_id(L['self']._modules['linear'], 126011600)     # base_output = self.linear(x)  # <ipython-input-57-053a79d3adfa>:29 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | | +- GuardManager: source=L['self']._modules['linear'].__dict__, accessed_by=GetGenericDictGuardAccessor\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | | | +- DICT_CONTAINS: not ___dict_contains('forward', L['self']._modules['linear'].__dict__)  # base_output = self.linear(x)  # <ipython-input-57-053a79d3adfa>:29 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | | | +- GuardManager: source=L['self']._modules['linear']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | | | | +- DICT_LENGTH: len(L['self']._modules['linear']._parameters) == 2            # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | | | | +- GuardManager: source=L['self']._modules['linear']._parameters['weight'], accessed_by=DictGetItemGuardAccessor('weight')\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['linear']._parameters['weight'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=True, size=[64, 128], stride=[128, 1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | | | | +- GuardManager: source=L['self']._modules['linear']._parameters['bias'], accessed_by=DictGetItemGuardAccessor('bias')\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | | | | | +- TENSOR_MATCH: check_tensor(L['self']._modules['linear']._parameters['bias'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=True, size=[64], stride=[1])  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | +- GuardManager: source=L['self']._parameters, accessed_by=DictGetItemGuardAccessor('_parameters')\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | +- DICT_LENGTH: len(L['self']._parameters) == 2                               # base_output = self.linear(x)  # <ipython-input-57-053a79d3adfa>:29 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | +- GuardManager: source=L['self']._parameters['lora_A'], accessed_by=DictGetItemGuardAccessor('lora_A')\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | | +- TENSOR_MATCH: check_tensor(L['self']._parameters['lora_A'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=True, size=[128, 8], stride=[8, 1])  # lora_update = torch.matmul(x, self.lora_A)  # <ipython-input-57-053a79d3adfa>:30 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | +- GuardManager: source=L['self']._parameters['lora_B'], accessed_by=DictGetItemGuardAccessor('lora_B')\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | | +- TENSOR_MATCH: check_tensor(L['self']._parameters['lora_B'], Parameter, DispatchKeySet(CUDA, BackendSelect, ADInplaceOrView, AutogradCUDA), torch.float16, device=0, requires_grad=True, size=[8, 64], stride=[64, 1])  # lora_update = torch.matmul(lora_update, self.lora_B)  # <ipython-input-57-053a79d3adfa>:31 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch')\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['torch'], 134245712056208)                  # lora_update = torch.matmul(x, self.lora_A)  # <ipython-input-57-053a79d3adfa>:30 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | +- GuardManager: source=G['torch'].matmul, accessed_by=GetAttrGuardAccessor(matmul)\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['torch'].matmul, 134245275791616)           # lora_update = torch.matmul(x, self.lora_A)  # <ipython-input-57-053a79d3adfa>:30 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_linear')\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'], 134244774415152)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F, accessed_by=GetAttrGuardAccessor(F)\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F, 134244774417392)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, accessed_by=GetAttrGuardAccessor(linear)\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_linear'].F.linear, 134245244646432)  # return F.linear(input, self.weight, self.bias)  # nn/modules/linear.py:125 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module'], accessed_by=DictGetItemGuardAccessor('__import_torch_dot_nn_dot_modules_dot_module')\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | +- ID_MATCH: ___check_obj_id(G['__import_torch_dot_nn_dot_modules_dot_module'], 134244777627776)  # base_output = self.linear(x)  # <ipython-input-57-053a79d3adfa>:29 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_hooks)\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_hooks  # base_output = self.linear(x)  # <ipython-input-57-053a79d3adfa>:29 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_hooks)\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_hooks  # base_output = self.linear(x)  # <ipython-input-57-053a79d3adfa>:29 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_forward_pre_hooks)\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_forward_pre_hooks  # base_output = self.linear(x)  # <ipython-input-57-053a79d3adfa>:29 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | +- GuardManager: source=G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks, accessed_by=GetAttrGuardAccessor(_global_backward_pre_hooks)\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] | | | | +- DICT_LENGTH: not G['__import_torch_dot_nn_dot_modules_dot_module']._global_backward_pre_hooks  # base_output = self.linear(x)  # <ipython-input-57-053a79d3adfa>:29 in forward\n",
            "V0219 02:23:08.058000 798 torch/_dynamo/guards.py:2321] [1/0] [__guards] \n",
            "V0219 02:23:09.063000 798 torch/_dynamo/guards.py:2346] [1/0] [__guards] Guard eval latency = 2.97 us\n",
            "I0219 02:23:09.068000 798 torch/_dynamo/pgo.py:636] [1/0] put_code_state: no cache key, skipping\n",
            "ERROR:__main__:⚠️ Error during compilation: '>' not supported between instances of 'Counter' and 'int'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dlfWiyhcBlfT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}